
maven是死知识：知道了就是会了 JVM、git、jekins；
活知识，（多线程、锁、事务、设计模式、分布式的设计--思维--实战）

多线程不一定就比单线程快，一定要考虑任务的性质和上下文切换，这也是为什么redis（redis设计模式单线程）比memcached多线程表现的要好，	
参考资料：https://www.cnblogs.com/zhimingyang/p/5702752.html
	首先最大的不同：synchronized是基于JVM层面实现的，而Lock是基于JDK层面实现的。曾经反复的找过synchronized的实现，可惜最终无果。
	但Lock却是基于JDK实现的，我们可以通过阅读JDK的源码来理解Lock的实现。
并发大师----Doug Lee。
老师推荐书籍：没基础看看《Java核心技术卷I》，有基础的看《Java并发编程实战》；

1/线程基本概念：{
	
	线程是超脱语言的
	单核：一个处理器
	双核、三核和四核
	一个CPU有几个核(处理器)，
	一个处理器同一时间只能运行一个线程，
		人的反应时间：0.1秒
		1秒==10亿纳秒，1秒=1000毫秒，1毫秒=1000微秒，1微秒=1000纳秒，
		计算机CPU:执行一调指令：0.6纳秒，
		
		时间片轮转机制：在进程之间切换
		上下文切换：一个上下文切换花费20000CPU周期，并发编程应减少上下文切换，
		
	
	CPU核心数和线程数的关系：{
		核心数:线程数=1:1  ;
		intel引入了超线程技术后--->使得核心数和线程数形成1:2的关系。
		
		多核心：{
			早期CPU受限于工艺、技术等限制，一个CPU处理器芯片只能封装一个处理核心，随着技术的发展，
				现在可以把2、4、6、8等多个处理核心封装在一个处理器芯片中，内部用高速总线连接以便协同工作，外观看着还是跟原来基本一样。
			也指单芯片多处理器( Chip Multiprocessors,简称CMP),CMP是由美国斯坦福大学提出的,其思想是将大规模并行处理器中的SMP(对称多处理器)集成到同一芯片内,
				各个处理器并行执行不同的进程。这种依靠多个CPU同时并行地运行程序是实现超高速计算的一个重要方向,称为并行处理。
		}
		
		多线程: Simultaneous Multithreading.简称SMT.让同一个处理器上的多个线程同步执行并共享处理器的执行资源。
				
		核心数、线程数：{
			目前主流CPU都是多核的。增加核心数目就是为了增加线程数,因为操作系统是通过线程来执行任务的,一般情况下它们是1:1对应关系,
				也就是说四核CPU一般拥有四个线程。但 Intel引入超线程技术后,使核心数与线程数形成1:2的关系。
		}
		
		
		早期CPU受限于工艺、技术等限制，一个CPU处理器芯片只能封装一个处理核心(就是一个处理器)，随着技术的发展，
				现在可以把2、4、6、8等多个处理核心封装在一个处理器芯片中，内部用高速总线连接以便协同工作，外观看着还是跟原来基本一样。
		核是指这个CPU处理器的数量，一个CPU的处理器越多这个CPU的性能就越好。电脑运行的速度就越快。
		一个处理核心(就是一个处理器)同一时间只能运行一个线程，某个CPU四核的：可以说四核八线程，
		
	}
	
	
	CPU时间片轮转机制--最古老、最公平、最简单的机制：{
		又称RR调度，会导致上下文切换
		//一次线程切换花费：5000到20000时钟周期左右，
		//上下文切换会影响性能：有时候觉得多线程还没有单线程快，
		CPU时间片轮转机制详细介绍：{
			
			我们平时在开发的时候，感觉并没有受cpu核心数的限制，想启动线程就启动线程，哪怕是在单核CPU上，为什么？这是因为操作系统提供了一种CPU时间片轮转机制。
			时间片轮转调度是一种最古老、最简单、最公平且使用最广的算法,又称RR调度。每个进程被分配一个时间段,称作它的时间片,即该进程允许运行的时间。
			百度百科对CPU时间片轮转机制原理解释如下:
			如果在时间片结束时进程还在运行,则CPU将被剥夺并分配给另一个进程。如果进程在时间片结束前阻塞或结来,则CPU当即进行切换。调度程序所要做的就是维护一张就绪进程列表,
				当进程用完它的时间片后,它被移到队列的末尾
			时间片轮转调度中唯一有趣的一点是时间片的长度。从一个进程切换到另一个进程是需要定时间的,包括保存和装入寄存器值及内存映像,更新各种表格和队列等。
				假如进程切( processwitch),有时称为上下文切换( context switch),需要5ms,再假设时间片设为20ms,则在做完20ms有用的工作之后,CPU将花费5ms来进行进程切换。
				CPU时间的20%被浪费在了管理开销上了。
			为了提高CPU效率,我们可以将时间片设为5000ms。这时浪费的时间只有0.1%。但考虑到在一个分时系统中,如果有10个交互用户几乎同时按下回车键,将发生什么情况?
				假设所有其他进程都用足它们的时间片的话,最后一个不幸的进程不得不等待5s才获得运行机会。
				多数用户无法忍受一条简短命令要5才能做出响应,同样的问题在一台支持多道程序的个人计算机上也会发
			结论可以归结如下:时间片设得太短会导致过多的进程切换,降低了CPU效率:而设得太长又可能引起对短的交互请求的响应变差。将时间片设为100ms通常是一个比较合理的折衷。
			在CPU死机的情况下,其实大家不难发现当运行一个程序的时候把CPU给弄到了100%再不重启电脑的情况下,其实我们还是有机会把它KⅢ掉的,我想也正是因为这种机制的缘故。
		}
	}	
		

	什么是进程和线程：{
		应用程序是死的，而进程是活的，

		进程：程序运行资源分配的最小单位，进程内部有多个线程，会共享这个进程的资源
			进程与进程是相互独立的，
		线程：CPU调度的最小单位，必须依赖进程而存在。
			线程不能独立运行，不拥有系统资源，进程申请的资源给线程用，
		进程：资源分配的最小单位；进程和进程是独立的，
		线程是CPU调度的最小单位，线程不能独立进程存在，必须依赖进程；
		
		进程是程序运行资源分配的最小单位：{
			进程是操作系统进行资源分配的最小单位,其中资源包括:CPU、内存空间、磁盘IO等,同一进程中的多条线程共享该进程中的全部系统资源,而进程和进程之间是相互独立的。
				进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位。
			进程是程序在计算机上的一次执行活动。当你运行一个程序,你就启动了一个进程。显然,程序是死的、静态的,进程是活的、动态的。进程可以分为系统进程和用户进程。
				凡是用于完成操作系统的各种功能的进程就是系统进程,它们就是处于运行状态下的操作系统本身,用户进程就是所有由你启动的进程。

		}
		
		线程是CPU调度的最小单位,必须依赖于进程而存在：{
			线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的、能独立运行的基本单位。线程自己基本上不拥有系统资源,
				只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。
		}
		
		线程无处不在：{
			任何一个程序都必须要创建线程,特别是Java不管任何程序都必须启动一个main函数的主线程; Java Web开发里面的定时任务、定时器、JSP和 Servlet、
				异步消息处理机制,远程访问接口RM等,任何一个监听事件, onclick的触发事件等都离不开线程和并发的知识。
		}
	
	}
	
	
	澄清并行和并发：{//并行度  并发量
		并行：同一时刻，可以同时处理事情的能力   			//8个窗口：并行度是8，  						  8核的并发度是8
			并行：同时运行的任务数；同时执行不同的任务是，
		并发：与单位时间相关，在单位时间内可以处理事情的能力//一个学生打饭30秒，8个窗口1分钟内的并发就是16。  时间片切换是100ms，1秒内的并发就是80，
			并发不能脱离时间单位的，并发交替执行，时间片轮转机制就是并发技术的应用，
		并行：同时运行的任务数；同时执行不同的任务，
		并发不能脱离时间单位的，并发交替执行，时间片轮转机制就是并发技术的应用，
		//线程共享进程的资源；
		澄清并行和并发详解：{
			我们举个例子,如果有条高速公路A上面并排有8条车道,那么最大的并行车辆就是8辆此条高速公路A同时并排行走的车辆小于等于8辆的时候,车辆就可以并行运行。
				CPU也是这个原理,一个CPU相当于一个高速公路A,核心数或者线程数就相当于并排可以通行的车道;而多个CPU就相当于并排有多条高速公路,而每个高速公路并排有多个车道。
			当谈论并发的时候一定要加个单位时间,也就是说单位时间内并发量是多少?离开了单位时间其实是没有意义的。
			俗话说,一心不能二用,这对计算机也一样,原则上一个CPU只能分配给一个进程,以便运行这个进程。我们通常使用的计算机中只有一个CPU,也就是说只有一颗心,
				要让它一心多用同时运行多个进程,就必须使用并发技术。实现并发技术相当复杂,最容易理解的是“时间片轮转进程调度算法”。
			综合来说：
				并发:指应用能够交替执行不同的任务,比如单CPU核心下执行多线程并非是同时执行多个任务,如果你开两个线程执行,就是在你几乎不可能察觉到的速度不断去切换这两个任务,
					已达到"同时执行效果",其实并不是的,只是计算机的速度太快,我们无法察觉到而已.
				并行:指应用能够同时执行不同的任务,例:吃饭的时候可以边吃饭边打电话,这两件事情可以同时执行
				两者区别:一个是交替执行,一个是同时执行.

		}
	
	}
			
			
	高并发编程的意义、好处和注意事项：{
		
		好处：充分利用cpu的资源；加快用户响应的时间；模块化、异步化处理、简单化。
		
		问题：
			线程共享资源，存在冲突；
			容易导致死锁；
			启用太多的线程，就有搞垮机器的可能
		
		高并发编程的意义、好处和注意事项：{
			由于多核多线程的CPU的诞生,多线程、高并发的编程越来越受重视和关注。多线程可以给程序带来如下好处。
			(1)充分利用CPU的资源
				从上面的CPU的介绍,可以看的出来,现在市面上没有CPU的内核不使用多线程并发机制的,特别是服务器还不止一个CPU,如果还是使用单线程的技术做思路,明显就out了。
					因为程序的基本调度单元是线程,并且一个线程也只能在一个CPU的一个核的一个线程跑,如果你是个i3的CPU的话,最差也是双核心4线程的运算能力:如果是一个线程的程序的话,
					那是要浪费3/4的CPU性能:如果设计一个多线程的程序的话,那它就可以同时在多个CPU的多个核的多个线程上跑,可以充分地利用CPU,减少CPU的空闲时间,发挥它的运算能力,提高并发量。
				就像我们平时坐地铁一样,很多人坐长线地铁的时候都在认真看书,而不是为了坐地铁而坐地铁,到家了再去看书,这样你的时间就相当于有了两倍。
					这就是为什么有些人时间很充裕,而有些人老是说没时间的一个原因,工作也是这样,有的时候可以并发地去做几件事情,充分利用我们的时间,CPU也是一样,也要充分利用。
			(2)加快响应用户的时间
				比如我们经常用的迅雷下载,都喜欢多开几个线程去下载,谁都不愿意用一个线程去下载,为什么呢?答案很简单,就是多个线程下载快啊。
				我们在做程序开发的时候更应该如此,特别是我们做互联网项目,网页的响应时间若提升1s,如果流量大的话,就能增加不少转换量。
					做过高性能web前端调优的都知道,要将静态资源地址用两三个子域名去加载,为什么?因为每多一个子域名,
					浏览器在加载你的页面的时候就会多开几个线程去加载你的页面资源,提升网站的响应速度。多线程,高并发真的是无处不在。
			(3)可以使你的代码模块化,异步化,简单化
				例如我们实现电商系统，下订单和给用户发送短信、邮件就可以进行拆分，将给用户发送短信、邮件这两个步骤独立为单独的模块，
					并交给其他线程去执行。这样既增加了异步的操作，提升了系统性能，又使程序模块化,清晰化和简单化。
				多线程应用开发的好处还有很多,大家在日后的代码编写过程中可以慢慢体会它的魅力。
		}		
			
	}
	
	
	多线程程序需要注意事项：{
		
		(1)线程之间的安全性
			从前面的章节中我们都知道,在同一个进程里面的多线程是资源共享的,也就是都可以访问同一个内存地址当中的一个变量。例如:若每个线程中对全局变量、静态变量只有读操作,而无写操作,
				一般来说,这个全局变量是线程安全的:若有多个线程同时执行写操作,一般都需要考虑线程同步,否则就可能影响线程安全。
		(2)线程之间的死锁
			为了解决线程之间的安全性引入了Java的锁机制,而一不小心就会产生Java线程死锁的多线程问题,因为不同的线程都在等待那些根本不可能被释放的锁,从而导致所有的工作都无法完成。
				假设有两个线程,分别代表两个饥饿的人,他们必须共享刀叉并轮流吃饭。他们都需要获得两个锁:共享刀和共享叉的锁。
			假如线程A获得了刀,而线程B获得了叉。线程A就会进入阻塞状态来等待获得叉,而线程B则阻塞来等待线程A所拥有的刀。这只是人为设计的例子,但尽管在运行时很难探测到,这类情况却时常发生
		(3)线程太多了会将服务器资源耗尽形成死机当机
			线程数太多有可能造成系统创建大量线程而导致消耗完系统内存以及CPU的“过渡切换”,造成系统的死机,那么我们该如何解决这类问题呢?
			某些系统资源是有限的,如文件描述符。多线程程序可能耗尽资源,因为每个线程都可能希望有一个这样的资源。如果线程数相当大,或者某个资源的侯选线程数远远超过了可用的资源数则最好使用资源池。
				一个最好的示例是数据库连接池。只要线程需要使用一个数据库连接,它就从池中取出一个,使用以后再将它返回池中。资源池也称为资源库。
			多线程应用开发的注意事项很多,希望大家在日后的工作中可以慢慢体会它的危险所在。

	}
	
	
	计算机资源：
		资源的范围更广：如CPU、内存、磁盘IO；
			
	操作系统限制线程数：Linux 线程数1000个，Windows 线程数2000个，---新线程需要栈空间，
	
	文件描述符：句柄
	
	
	
}



2/认识Java里的线程-----线程的启动方式和线程终止：{
	
	//java里的程序天生就是多线程的 ，那么有三种新启线程的方式？线程又是如何终止的？
		
	如何证明里的程序天生就是多线程的：{
		public class OnlyMain {
			public static void main(String[] args) {
				ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean();
				ThreadInfo[] threadInfos = threadMXBean.dumpAllThreads(false, false);
				for (ThreadInfo threadInfo :threadInfos) {
					System.out.println("["+ threadInfo.getThreadId() +"]" + "" + threadInfo.getThreadName());
				}
				//[6]Monitor Ctrl-Break                          //监控Ctrl-Break中断信号的
				//[5]Attach Listener 获取当前程序运行的各种信息  //内存dump，线程dump，类信息统计，获取系统属性等
				//[4]Signal Dispatcher 分发给虚拟机信号的线程    // 分发处理发送给JVM信号的线程
				//[3]Finalizer 调用对象的Finalizer方法
				//[2]Reference Handler:清除引用
				//[1]main  //main线程，用户程序入口
			}
		}
		没有GC线程：程序比较短，没有体现出来，
		
		有个对象要进行回收或相关资源的释放，可以调用Object中的Finalizer方法，重写，但有时这个方法并不靠谱，有时并不会执行，这个方法是一个守护线程，
	}
	
	有个对象要进行回收或相关资源的释放，可以调用Object中的Finalizer方法，重写，但有时这个方法并不靠谱，有时并不会执行，这个方法是一个守护线程，
		
		启动线程一般意义上的三种方法---但实际上只有两种新启方式：{
			类Thread
			
			接口Runnable
				没有返回值
				为什么有一个类Thread，还要提供接口Runnable：因为java是单继承的，只能继承一个，不能继承多个，此时就要通过实现来扩展，
			接口Callable
				有返回值
		}
			
		三种新启线程的方式：{
				public class ThreadApplicaton {
					/*extends Thread*/
					//第一种新启线程方式
					/*implements Runnable*/
					//第二种新启线程方式
					private static class UseRunnable implements Runnable{
						@Override
						public void run() {
							System.out.println("Runnable是没有返回值的，");
						}
					}
					
					/*implements Callable*/
					//第三种新启线程方式
					private static class UseCallable implements Callable{
						@Override
						public String call() throws Exception {
							System.out.println("Callable是有返回值的，");
							return "UseCallable";
						}
					}
					
					public static void main(String[] args) throws InterruptedException, ExecutionException {
						UseRunnable useRunnable = new UseRunnable();
						new Thread(useRunnable).start();
						
						UseCallable useCallable = new UseCallable();
						FutureTask<String> stringFutureTask = new FutureTask<>(useCallable);
						new Thread(stringFutureTask).start();
						System.out.println(stringFutureTask.get());
					}
					
				}
				
		}
		
	
	正真实现多线程有两种方式：Thread和Runnable，Runnable和Callable可以归为一类；
		启动线程的方式有：
			1/X extends Thread;，然后X.run
			2/X implements  Runnable；然后交给Thread运行

		Thread和Runnable区别？
			Thread才是java里对线程的唯一抽象，Runnable只是对任务、业务逻辑的抽象。
			Thread可以接受任意一个Runnable的实例并执行。
	
	
	线程终结：{
		
		有开始就有结束，怎么样才能让Java里的线程安全停止工作呢
			线程自然终止：自然执行完或抛出未处理异常
			1/废弃掉的方法：
				stop()，resume(),suspend()挂起：已不建议使用，不是不能用（如果明确知道线程在做什么任务？可以用的）
				stop()过于强势，会导致线程不会正确释放资源，
				suspend()容易导致死锁。
			
		java线程是协作式，而非抢占式。
			2/常用的方法：
			
			2.1/interrupt()对线程的中断，
				调用一个线程的interrupt() 方法中断一个线程，不是真正中断一个线程只是发出一个中断标志位；
				这个方法并不是强行关闭这个线程，只是跟这个线程打个招呼，将线程的中断标志位置为true：
					线程可以立即停止，也可以不理会由线程决定，线程是否中断，由线程本身决定。---由此得出JDK中的线程是协作式的，不是抢占式的；
			
			2.2/isInterrupted() 判定当前线程是否处于中断状态；
			2.3/static方法interrupted() 判定当前线程是否处于中断状态，把中断标志位由true改为false,清除中断标志位，同时将中断标志位改为false；
			2.4/方法里如果抛出InterruptedException中断异常，线程的中断标志位会被复位成false，如果确实是需要中断线程，要求我们自己在catch语句块里再次调用interrupt()。
			
			2.5/不建议自定义一个取消标志位来中止线程的运行。
				有时候，自己定义一个boolean值去中断一个线程，采用中断标志位中断一个线程，不建议这样做，因为程序中有sleep,wait方法时，被挂起，出现异常，
			
			死锁是不会理会中断的，
	}
	
	
}



3/对Java里的线程再多一点点认识------线程状态：{
	
	
	深入理解run()和start() ：{
		start：启动一个线程，
			start方法是真真正正启动一个线程，多次启动会报异常，因为start方法时，会回收线判断当前线程状态，
			start方法允许只能调用一次，否则报错，调用两次就报错，
		run：是方法里面的业务逻辑，
			可以脱离线程单独调用，
			普通类普通方法，
		深入理解run()和start() ：{
			Thread类是Java里对线程概念的抽象，可以这样理解：我们通过new Thread()其实只是new出一个Thread的实例，还没有操作系统中真正的线程挂起钩来。
				只有执行了start()方法后，才实现了真正意义上的启动线程。
			start()方法让一个线程进入就绪队列等待分配cpu，分到cpu后才调用实现的run()方法，start()方法不能重复调用，如果重复调用会抛出异常。
			而run方法是业务逻辑实现的地方，本质上和任意一个类的任意一个成员方法并没有任何区别，可以重复执行，也可以被单独调用。
		}
	
	}
	
		
	线程状态：{
		
		新建：这是还是jdk里的一个实例，调用start后，没有运行，而是进行就绪状态；
		
		就绪：处于可运行状态，
			什么时候才能运行呢？
				CPU分配时间了(时间片轮转机制)，才开始运行。
				拿到执行权：运行，CPU时间片轮转机制，
				
		运行：
			join方法进入运行状态，
			yield()让出一个CPU执行权，由于分配的时间片到期，线程由运行进入就绪状态，
			
		死亡：run结束：
			运行之后正常死亡，或者stop方法也会死亡，或者线程本身是一个守护线程死亡(其他非守护线程执行完了，守护线程也就死亡了)。
		
		阻塞：
			调用sleep方法，进入阻塞状态，当前线程被操作系统挂起来了，不在分配cpu等资源了。
				sleep时间到了，线程从睡眠中唤醒，进入就绪状态；
			调用wait方法，同样进入阻塞状态，当有别的线程通知notify你不需要等待了，进入就绪状态；
			调用interrupt是从阻塞进入就绪状态，interrupt方法只是改标志位，
			
	}
	
	
	yield()方法：
	yield():使当前线程让出CPU占有权，操作系统会将让出的时间片在几个线程之间重新进行分配，所有线程都有可能被选中，
	其他的线程相关方法：{
		yield()方法：
			使当前线程让出CPU占有权，
			但让出的时间是不可设定的。
			也不会释放锁资源。
			注意：因为又不是每个线程都需要这个锁的，而且执行yield( )的线程不一定就会持有锁。
		所有执行yield()的线程有可能在进入到就绪状态后会被操作系统再次选中马上又被执行。
		wait()/notify()/notifyAll()：后面会单独讲述，
	}
	yield():让出cpu的执行权，但不会让出锁，不会释放锁，下次抢到线程的仍然有可能是我，
	
	
	
		
	sleep休眠，下次抢到锁的就没有我了， 
	
	join方法：插队
			面试：怎么保证两个线程顺序执行，使用join方法，join让线程变成了串行，
	join方法：{
		把指定的线程加入到当前线程，可以将两个交替执行的线程合并为顺序执行。比如在线程B中调用了线程A的Join()方法，直到线程A执行完毕后，才会继续执行线程B。(此处为常见面试考点)
	}
	
	
	
	
	线程的优先级：{
		优先级的范围：1~10，不做设置，缺省为5，默认值是5，
		优先级高的线程分配时间片的数量多于优先级低的线程，如何设置 .setPriority(),
		线程的优先级不是可靠可用的东西，不一定会起到多大的作用，没有多大意义。
		
		线程的优先级{
			在Java线程中，通过一个整型成员变量priority来控制优先级，优先级的范围从1~10，在线程构建的时候可以通过setPriority(int)方法来修改优先级，默认优先级是5，
				优先级高的线程分配时间片的数量要多于优先级低的线程。
			设置线程优先级时，针对频繁阻塞（休眠或者I/O操作）的线程需要设置较高优先级，而偏重计算（需要较多CPU时间或者偏运算）的线程则设置较低的优先级，
				确保处理器不会被独占。在不同的JVM以及操作系统上，线程规划会存在差异，有些操作系统甚至会忽略对线程优先级的设定。

		}
		
	}
	
	
	 
	守护线程：{
		setDaemon();
		和主线程是同生共死的，主线程退出，守护一定会退出，
		GC，垃圾回收线程也属于守护线程，
		finally不能保证一定执行，用户线程一定会执行，守护线程不一定会执行，
		用户线程先结束，守护线程后结束，
		用户线程里的finally会执行，
		用途：后台调度、支持性工作、做底层框架会用到；
		守护线程：{
			
			Daemon（守护）线程是一种支持型线程，因为它主要被用作程序中后台调度以及支持性工作。这意味着，当一个Java虚拟机中不存在非Daemon线程的时候，Java虚拟机将会退出。
				可以通过调用Thread.setDaemon(true)将线程设置为Daemon线程。我们一般用不上，比如垃圾回收线程就是Daemon线程。
			Daemon线程被用作完成支持性工作，但是在Java虚拟机退出时Daemon线程中的finally块并不一定会执行。
				在构建Daemon线程时，不能依靠finally块中的内容来确保执行关闭或清理资源的逻辑。

		}
	}	
	
	
	//徐尔谦
	
}



4/什么是线程间的共享？----synchronized内置锁--volatile--ThreadLocal：{
	
	
	深入理解ReentrantLock:
		实现：首先最大的不同：synchronized是基于JVM层面实现的，而Lock是基于JDK层面实现的。曾经反复的找过synchronized的实现，可惜最终无果。
			  但Lock却是基于JDK实现的，我们可以通过阅读JDK的源码来理解Lock的实现。
		使用：对于使用者的直观体验上Lock是比较复杂的，需要lock和realse，如果忘记释放锁就会产生死锁的问题，所以，通常需要在finally中进行锁的释放。
			  但是synchronized的使用十分简单，只需要对自己的方法或者关注的同步对象或类使用synchronized关键字即可。但是对于锁的粒度控制比较粗，同时对于实现一些锁的状态的转移比较困难。
		优化：在JDK1.5/之后synchronized引入了偏向锁，轻量级锁和重量级锁，从而大大的提高了synchronized的性能，同时对于synchronized的优化也在继续进行。期待有一天能更简单的使用java的锁。
		在以前不了解Lock的时候，感觉Lock使用实在是太复杂，但是了解了它的实现之后就被深深吸引了。
		Lock的实现主要有ReentrantLock、ReadLock和WriteLock，后两者接触的不多，所以简单分析一下ReentrantLock的实现和运行机制。
		参考资料：https://www.cnblogs.com/zhimingyang/p/5702752.html
		
	
	
	
	
	线程需要分配栈空间，执行run方法，
	启动多个线程：达到数据共享，协同处理，
	同一个进程的资源由多个线程共享，多个线程可以访问某个对象或成员变量，	
		
	synchronized内置锁：{
		
		java提供的，锁的具体的对象，class文件头上有个标志位，
		保证：同一时间只能由一个线程访问某个资源，
			方法加锁和代码加锁，都是对象锁，谁先拿到方法上的锁，谁先执行，
			同步方法和同步块，this和obj一样，对当前对象加锁，
		用处
			对象锁：方法上没有static synchronized修饰的，---对象锁锁的是new的实例，和类锁以同时运行的，不同对象锁也是可以同时运行的
			类锁：方法上有static synchronized 修饰的，是一个class对象，
			对象锁和类锁可以并行运行，
			类的class对象		//有点问题
			类的静态成员变量，	//有点问题
			都是虚拟机里的某个对象，两个线程可以并行执行；但是抢的是同一个对象，就不能并行了，
			
		synchronized即可确保可见性，有可确保原子性；
		synchronized本质锁得每个类在虚拟机中的class对象,锁的某个具体的对象，	
		synchronized锁的对象，是不可变的
		
		多线程：
			用于：并发执行、模块化、
			
		执行某段代码的时候，一定要先拿到这个对象的锁，才能执行，	
		
		synchronized内置锁：{
			线程开始运行的时候，都拥有自己的栈空间，就如同一个脚本一样，按照既定代码一步一步地执行，直到终止。
				但是，如果，每个运行中线程是孤立地运行，那么就没有一点儿价值，或者说价值很少，如果多个线程能够相互配合完成工作，包括数据之间的共享、协同处理事情，这将会带来巨大的价值。
			java支持多个线程同时访问一个对象或对象的成员变量，关键字synchronized可以修饰方法或者以同步块的形式来进行使用，它主要确保多个线程在同一时刻，只能有一个线程处于方法或同步块中，
				它保证了线程对变量的访问的可见性和排他性，又称为内置锁机制。
		}
		
		对象锁和类锁：{
			对象锁是用于对象实例方法，或者一个对象实例上的，类锁是用于类的静态方法或者一个类的class对象上的；//类锁也称class对象锁
				类的对象实例可以有很多个，但是每个类只有一个class对象，所以不同实例的对象锁是互不干扰的，但是每个类只有一个类锁。
			
			有一点需要注意：类锁只是一个概念上的东西，并不是真实存在的，类锁其实锁的是每个类的对应的class对象。
				类锁和对象锁之间也是互不干扰的。
				
		}
		
		synchronized锁的是对象，锁的对象不会发生变化，基本类型是锁不住的，
		
		错误的加锁和原因分析：{
			本质上是返回了一个新的Integer对象。也就是每个线程实际加锁的是不同的Integer对象。
		}
		
	}
	
	
	volatile：关键字，最轻量的同步机制；{
		volatile：保证可见性，不保证原子性，不能保证线程安全。
		volatile使用场景：适合一写多读的场景，
		volatile的最大作用/适用范围：只有一个线程写，多个线程读，
		方法：
			get：将自己的值置为无效，从主内存取出作为自己的值，
			set：将自己的值写入主内存，
		
		
		volatile不能保证多线程下的安全，
		
		private volatile static boolean ready;
		private static int number;
			不加volatile时，子线程无法感知主线程修改了ready的值，从而不会退出循环，而加了volatile后，子线程可以感知主线程修改了ready的值，迅速退出循环。
			但是volatile不能保证数据在多个线程下同时写时的线程安全，

	
	
	}
	
	
	
	ThreadLocal:线程变量{
		ThreadLocal和Synchonized都用于解决多线程并发访问。可是ThreadLocal与synchronized有本质的差别。
			synchronized是利用锁的机制，使变量或代码块在某一时刻仅仅能被一个线程访问。
			而ThreadLocal为每个线程都提供了变量的副本，使得每个线程在某一时间访问到的并非同一个对象，这样就隔离了多个线程对数据的数据共享。
		
		ThreadLocal为线程提供变量的副本，实现线程的隔离，
		spring中事务用了ThreadLocal，
	
		方法：
			初始化
			get：将自己的值置为无效，从主内存取出作为自己的值，
			set：将自己的值写入主内存，
			remove
		确保线程使用自己的那份拷贝，
		空间换取线程的安全性，
		不保证同步，保证隔离，
		
		
		ThreadLocal为每个线程都提供了变量的副本，使得每个线程在某一时间访问到的并非同一个对象，这样就隔离了多个线程对数据的数据共享。
		每个线程拥有独立的成员变量：这个变量就是ThreadLocalMap，ThreadLocalMap是某个线程独有的，只有一个；
		一个线程可能拥有ThreadLocal型的变量不止一个，
		ThreadLocalMap有很多Entry型的数组，
		
		当一个线程要访问某一个ThreadLocal变量，根据当前线程获取这个线程独有的ThreadLocalMap，把要访问的ThreadLocal变量作为key，当前线程的值作为value，保存到Entry数组里，
			当这个线程又去访问另外一个ThreadLocal变量，又把另外一个ThreadLocal变量作为key，当前线程的值作为value，保存到另外一个Entry数组里，所以和多线程没有关系。
			
			
		
		
			//remove 清除引用，能解决内存泄漏的问题，
		ThreadLocal引发内存泄漏的问题：{
			
			预备知识：
				强引用就是指在程序代码之中普遍存在的，类似“Object obj=new Object（）”这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象实例。
				软引用是用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象实例列进回收范围之中进行第二次回收。
					如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK 1.2之后，提供了SoftReference类来实现软引用。
				弱引用也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象实例只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，
					都会回收掉只被弱引用关联的对象实例。在JDK 1.2之后，提供了WeakReference类来实现弱引用。
				虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象实例是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。
					为一个对象设置虚引用关联的唯一目的就是能在这个对象实例被收集器回收时收到一个系统通知。在JDK 1.2之后，提供了PhantomReference类来实现虚引用。
				强引用：JVM宁愿OOM，也不会去回收，
				软引用：对空间不足，就回收
				弱引用：GC就回收
				虚引用：随时被回收，
			
			实例应用----->>-Xmx256m：
				第一次幅度控制在25M左右，
					new LocalVariable();//第一次启动
					System.out.println("use local variable!!!");
				第二次幅度控制在200M左右，
					localVariable.set(new LocalVariable());//第二次启动
                    System.out.println("use local variable!!!");
				第三次幅度控制在25M左右，
					localVariable.set(new LocalVariable());
                    System.out.println("use local variable!!!");
                    localVariable.remove();//第三次启动
				
			分析：
				根据我们前面对ThreadLocal的分析，我们可以知道每个Thread维护一个ThreadLocalMap，这个映射表的key是ThreadLocal实例本身，value是真正需要存储的Object，
					也就是说ThreadLocal 本身并不存储值，它只是作为一个key来让线程从ThreadLocalMap获取value。仔细观察ThreadLocalMap，这个map是使用ThreadLocal的弱引用作为Key的，
					弱引用的对象在 GC 时会被回收。
					
					stack             虚引用                   heap
					thread local ref=====================>thread local<-------------------------key
																					===========>value======>My 50MB value
					current thread ref=================>current thread===>Map ==========>       Entry
						key是弱引用；如果是强引用，key和current thread ref仍然会存在强引用，当GC的时候，引发内存泄漏，
						value不用弱引用，是强引用：万一thread local还没有被回收，key也在，value用弱引用，被回收了，这样通过key就访问不到value了，
					remove会把强引用、Entry和value全部清除掉，调用expungeStaleEntry方法清除；
					而get,set方法不是每次都调用expungeStaleEntry方法清除；
				图中的虚线表示弱引用。
				这样，当把threadlocal变量置为null以后，没有任何强引用指向threadlocal实例，所以threadlocal将会被gc回收。这样一来，ThreadLocalMap中就会出现key为null的Entry，
					就没有办法访问这些key为null的Entry的value，如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：
					Thread Ref -> Thread -> ThreaLocalMap -> Entry -> value，而这块value永远不会被访问到了，所以存在着内存泄露。
				只有当前thread结束以后，current thread就不会存在栈中，强引用断开，Current Thread、Map value将全部被GC回收。
					最好的做法是不在需要使用ThreadLocal变量后，都调用它的remove()方法，清除数据。
				其实考察ThreadLocal的实现，我们可以看见，无论是get()、set()在某些时候，调用了expungeStaleEntry方法用来清除Entry中Key为null的Value，但是这是不及时的，
					也不是每次都会执行的，所以一些情况下还是会发生内存泄露。只有remove()方法中显式调用了expungeStaleEntry方法。

				从表面上看内存泄漏的根源在于使用了弱引用，但是另一个问题也同样值得思考：为什么使用弱引用而不是强引用？
					下面我们分两种情况讨论：
						key 使用强引用：引用ThreadLocal的对象被回收了，但是ThreadLocalMap还持有ThreadLocal的强引用，如果没有手动删除，
							ThreadLocal的对象实例不会被回收，导致Entry内存泄漏。
						key 使用弱引用：引用的ThreadLocal的对象被回收了，由于ThreadLocalMap持有ThreadLocal的弱引用，即使没有手动删除，ThreadLocal的对象实例也会被回收。
							value在下一次ThreadLocalMap调用set，get，remove都有机会被回收。
						比较两种情况，我们可以发现：由于ThreadLocalMap的生命周期跟Thread一样长，如果都没有手动删除对应key，都会导致内存泄漏，但是使用弱引用可以多一层保障。
						因此，ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。
			总结
				JVM利用设置ThreadLocalMap的Key为弱引用，来避免内存泄露。
				JVM利用调用remove、get、set方法的时候，回收弱引用。
				当ThreadLocal存储很多Key为null的Entry的时候，而不再去调用remove、get、set方法，那么将导致内存泄漏。
				使用线程池+ ThreadLocal时要小心，因为这种情况下，线程是一直在不断的重复运行的，从而也就造成了value可能造成累积的情况。

				
		}
			
			
	
		什么时候线程不安全？{
			为什么每个线程都输出5？难道他们没有独自保存自己的Number副本吗？为什么其他线程还是能够修改这个值？仔细考察ThreadLocal和Thead的代码，
				我们发现ThreadLocalMap中保存的其实是对象的一个引用，这样的话，当有其他线程对这个引用指向的对象实例做修改时，
				其实也同时影响了所有的线程持有的对象引用所指向的同一个对象实例。
				这也就是为什么上面的程序为什么会输出一样的结果：5个线程中保存的是同一Number对象的引用，在线程睡眠的时候，其他线程将num变量进行了修改，
				而修改的对象Number的实例是同一份，因此它们最终输出的结果是相同的。
			而上面的程序要正常的工作，应该的用法是让每个线程中的ThreadLocal都应该持有一个新的Number对象。
		}
		
	


	
}

	
	

}

	Object obj = new Object();
	new Object();一个实例
	obj是一个引用，             
5/线程间协作---线程之间是协作式的，不是抢占式的：{
	
	
	线程协作:线程相互配合完成某项工作：{
		比如：一个线程修改了一个对象的值，而另一个线程感知到了变化，然后进行相应的操作，整个过程开始于一个线程，而最终执行又是另一个线程。
			前者是生产者，后者就是消费者，这种模式隔离了“做什么”（what）和“怎么做”（How），简单的办法是让消费者线程不断地循环检查变量是否符合预期在while循环中设置不满足的条件，
			如果条件满足则退出while循环，从而完成消费者的工作。却存在如下问题：
			1/难以确保及时性。
			2/难以降低开销。如果降低睡眠的时间，比如休眠1毫秒，这样消费者能更加迅速地发现条件变化，但是却可能消耗更多的处理器资源，造成了无端的浪费。

	}
	
	等待/通知机制：{
		
		是指一个线程A调用了对象O的wait()方法进入等待状态，而另一个线程B调用了对象O的notify()或者notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而执行后续操作。
			上述两个线程通过对象O来完成交互，而对象上的wait()和notify/notifyAll()的关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。
			notify()：
				通知一个在对象上等待的线程,使其从wait方法返回,而返回的前提是该线程获取到了对象的锁，没有获得锁的线程重新进入WAITING状态。
			notifyAll()：
				通知所有等待在该对象上的线程
			wait()
				调用该方法的线程进入 WAITING状态,只有等待另外线程的通知或被中断才会返回.需要注意,调用wait()方法后,会释放对象的锁
			wait(long)
				超时等待一段时间,这里的参数时间是毫秒,也就是等待长达n毫秒,如果没有通知就超时返回
			wait (long,int)
				对于超时时间更细粒度的控制,可以达到纳秒

	}
	
	等待和通知的标准范式：{
		
		等待方遵循如下原则。
			1/获取对象的锁。
			2/如果条件不满足，那么调用对象的wait()方法，被通知后仍要检查条件。
			3/条件满足则执行对应的逻辑。
		通知方遵循如下原则。
			1/获得对象的锁。
			2/改变条件。
			3/通知所有等待在对象上的线程。
		
		在调用wait（）、notify()系列方法之前，线程必须要获得该对象的对象级别锁，即只能在同步方法或同步块中调用wait（）方法、notify()系列方法，进入wait（）方法后，
			当前线程释放锁，在从wait（）返回前，线程与其他线程竞争重新获得锁， 执行notify()系列方法的线程退出调用了notifyAll的synchronized代码块的时候后，他们就会去竞争。
			如果其中一个线程获得了该对象锁，它就会继续往下执行，在它退出synchronized代码块，释放锁后，其他的已经被唤醒的线程将会继续竞争获取该锁，一直进行下去，
			直到所有被唤醒的线程都执行完毕。

	}
	
	
	notify和notifyAll应该用谁：{
		尽可能用notifyall()，谨慎使用notify()，因为notify()只会唤醒一个线程，我们无法确保被唤醒的这个线程一定就是我们需要唤醒的线程，具体表现参见代码。
	}
	
	
	notify：一个条件下是没有问题的，
	notifyAll
	

	一个对象调用wait方法会释放它所持有的锁，
	notify/notifyAll直到代码走完才会释放锁，
	notify/notifyAll放到代码最后一行，
	
	
	线程间协作：
		轮询：难以保证及时性，资源开销很大，
	等待和通知：
		sleep是线程的方法，
		wait()  / notify/notifyAll 是对象上的方法，
		
		
	等待和通知
		wait()    对象上的方法
		
		notify/notifyAll  对象上的方法

		等待和通知的标准范式
			等待方：
				1/获取对象的锁；
				2/循环里判断条件是否满足，不满足调用wait方法，
				3/条件满足执行业务逻辑
			通知方来说
				1/获取对象的锁；
				2/改变条件
				3/通知所有等待在对象的线程

		//唤醒一个线程用notify还是notifyAll?
		notify和notifyAll应该用谁？
			应该尽量使用notifyAll，使用notify因为有可能发生信号丢失的的情况
			
		
		
		
	mysql连接数支持数150，两三百左右，
	//不能让线程无限制等待的，--等待超时的范式
	等待超时模式实现一个连接池:{
		
		假设  等待时间时长为T，当前时间now+T以后超时

		long  overtime = now+T;
		long remain = T;//等待的持续时间
		while(result不满足条件&& remain>0){
			wait(remain);
			remain = overtime – now;//等待剩下的持续时间
		}
		return result;
		//等待超时的基本范式；
		
	}
	c3p0，dbcp与druid：虽然高效，但实际没有脱离等待超时连接池，
	
	
	//UseJoin代码
	join()方法
		面试点
			线程A如何保证在线程B执行完之后执行，
				答：用join方法，或countDownLatch方法，
			线程A，执行了线程B的join方法，线程A必须要等待B执行完成了以后，线程A才能继续自己的工作--//join的最大用处；
			
	//面试题，	
	调用yield() 、sleep()、wait()、notify()等方法对锁有何影响？ {
	
		yield()：我当前的工作做完了，不想睡眠，可以把cpu的控制权交出去，由操作系统调度，来选择，下一个时间片运行的是哪一个线程，
		线程在执行yield()以后，持有的锁是不释放的，//会选择它
	
		sleep()方法被调用以后，持有的锁是不释放的，//不会选择它
		
		调动方法之前，必须要持有锁。调用了wait()方法以后，锁就会被释放，当wait方法返回的时候，线程会重新持有锁
		
		调动方法之前，必须要持有锁，调用notify()方法本身不会释放锁的，一般放到方法中的最后一行，
	}

	推荐书籍：
		java核心技术 卷1
		java并发编程实战

	
	
}

//以上是基础部分的
//多线程不一定就比单线程快，一定要考虑任务的性质和上下文切换，这也是为什么redis（redis设计模式单线程）比mocached多线程表现的要好，
//业务模块和异步可以考虑使用多线程，
//根据计算任务的性质
//用了锁都会造成死锁，

作业：使用fork-join实现一个归并排序，

6/线程的并发工具类：{
	Fork-Join:分而治之和工作密取；
	分而治之：基于算法思想，如快排、归并排序等算法、二分查找；map reduce，
	分而治之：把大问题分割成相同的小问题，小问题无关联，
			有关联的是动态规划，
	十大计算机经典算法：
	大数据中的分而治之的思想：map reduce,内存计算，
	
	Fork-Join：标准范式， 
	
	Fork-Join的实战：
	Fork-Join的线程池
	
	Fork-Join：{
		java下多线程的开发可以我们自己启用多线程，线程池，还可以使用forkjoin，forkjoin可以让我们不去了解诸如Thread,Runnable等相关的知识，只要遵循forkjoin的开发模式，
			就可以写出很好的多线程并发程序；
		
		分而治之
			同时forkjoin在处理某一类问题时非常的有用，哪一类问题？分而治之的问题。
			十大计算机经典算法：快速排序、堆排序、归并排序、二分查找、线性查找、深度优先、广度优先、Dijkstra、动态规划、朴素贝叶斯分类，
				有几个属于分而治之？3个，快速排序、归并排序、二分查找，还有大数据中M/R都是。 
			分治法的设计思想是：将一个难以直接解决的大问题，分割成一些规模较小的相同问题，以便各个击破，分而治之。
			分治策略是：对于一个规模为n的问题，若该问题可以容易地解决（比如说规模n较小）则直接解决，否则将其分解为k个规模较小的子问题，
				这些子问题互相独立且与原问题形式相同(子问题相互之间有联系就会变为动态规范算法)，递归地解这些子问题，然后将各子问题的解合并得到原问题的解。
				这种算法设计策略叫做分治法。
			归并排序
				归并排序是建立在归并操作上的一种有效的排序算法。
					该算法是采用分治法的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。
				若将两个有序表合并成一个有序表，称为2-路归并，与之对应的还有多路归并。
				对于给定的一组数据，利用递归与分治技术将数据序列划分成为越来越小的半子表，在对半子表排序后，再用递归方法将排好序的半子表合并成为越来越大的有序序列。
				为了提升性能，有时我们在半子表的个数小于某个数（比如15）的情况下，对半子表的排序采用其他排序算法，比如插入排序。
		
		Fork-Join原理：
			Fork/Join框架：就是在必要的情况下，将一个大任务，进行拆分(fork)成若干个小任务(拆分到不可再拆分时)，再将一个个的小任务运算的结果进行join汇总。
		工作密取
			即当前线程的Task已经全被执行完毕，则自动取到其他线程的Task池中取出Task继续执行。
			ForkJoinPool中维护着多个线程（一般为CPU核数）在不断地执行Task，每个线程除了执行自己职务内的Task之外，
				还会根据自己工作线程的闲置情况去获取其他繁忙的工作线程的Task，如此一来就能能够减少线程阻塞或是闲置的时间，提高CPU利用率。
				
				
				
		用例：Excel，统计sql，
		
		Fork/Join的同步用法同时演示返回结果值：统计整形数组中所有元素的和：
			实战例子效果：	
				the count is -1904990473spent time: 53230ms；//单线程
				the count is -1894655224spent time: 12983ms：//多线程
		Fork/Join的异步用法同时演示不要求返回值：遍历指定目录（含子目录）寻找指定类型文件：
			

		
	}
	
	
	
	常用的并发工具类
		CountDownLatch：闭锁，发令枪：让线程同时进行；{
			
			CountDownLatch，在预备线程，初始化线程执行完了，在执行
			初始化工作（放在初始化线程里去做）
			初始化线程和计数器没有关系：一个线程可以有多个计数器，
			线程数和计数器数是不一致的，
			作用：是一组线程等待其他的线程完成工作以后在执行，加强版join
			await用来等待，countDown负责计数器的减一，
			
		}	
		
		
		CyclicBarrier：循环屏障{
			让一组线程达到某个屏障，被阻塞，一直到组内最后一个线程达到屏障时，屏障开放，所有被阻塞的线程会继续运行CyclicBarrier(int parties)
			CyclicBarrier(int parties, Runnable barrierAction)，屏障开放，barrierAction定义的任务会执行
			
			CyclicBarrier的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，
				屏障才会开门，所有被屏障拦截的线程才会继续运行。CyclicBarrier默认的构造方法是CyclicBarrier（int parties），其参数表示屏障拦截的线程数量，
				每个线程调用await方法告诉CyclicBarrier我已经到达了屏障，然后当前线程被阻塞。
			CyclicBarrier还提供一个更高级的构造函数CyclicBarrier（int parties，Runnable barrierAction），用于在线程到达屏障时，优先执行barrierAction，方便处理更复杂的业务场景。
			CyclicBarrier可以用于多线程计算数据，最后合并计算结果的场景。
			
		}
			
			
		CountDownLatch和CyclicBarrier辨析：{
			1、countdownlatch放行由第三者控制，CyclicBarrier放行由一组线程本身控制
			2、countdownlatch放行条件 >= 线程数，CyclicBarrier放行条件=线程数
		
			CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以反复使用。
			CountDownLatch.await一般阻塞工作线程，所有的进行预备工作的线程执行countDown，而CyclicBarrier通过工作线程调用await从而自行阻塞，
				直到所有工作线程达到指定屏障，再大家一起往下走。
			在控制多个线程同时运行上，CountDownLatch可以不限线程数量，而CyclicBarrier是固定线程数。
			同时，CyclicBarrier还可以提供一个barrierAction，合并多线程计算结果。

		}
		
		
		
		Semaphore：{
			控制同时访问某个特定资源的线程数量，用在流量控制
			
			Semaphore（信号量）是用来控制同时访问特定资源的线程数量，它通过协调各个线程，以保证合理的使用公共资源。
				应用场景Semaphore可以用于做流量控制，特别是公用资源有限的应用场景，比如数据库连接。假如有一个需求，要读取几万个文件的数据，因为都是IO密集型任务，
				我们可以启动几十个线程并发地读取，但是如果读到内存后，还需要存储到数据库中，而数据库的连接数只有10个，这时我们必须控制只有10个线程同时获取数据库连接保存数据，
				否则会报错无法获取数据库连接。这个时候，就可以使用Semaphore来做流量控制。。Semaphore的构造方法Semaphore（int permits）接受一个整型的数字，表示可用的许可证数量。
				Semaphore的用法也很简单，首先线程使用Semaphore的acquire()方法获取一个许可证，使用完之后调用release()方法归还许可证。还可以用tryAcquire()方法尝试获取许可证。
			Semaphore还提供一些其他方法，具体如下。
				•intavailablePermits()：返回此信号量中当前可用的许可证数。
				•intgetQueueLength()：返回正在等待获取许可证的线程数。
				•booleanhasQueuedThreads()：是否有线程正在等待获取许可证。
				•void reducePermits（int reduction）：减少reduction个许可证，是个protected方法。
				•Collection getQueuedThreads()：返回所有等待获取许可证的线程集合，是个protected方法。
		}	
		
		
		Exchange：{
			用处不是很大，主要用于两个线程之间进行数据交换，
			两个线程间的数据交换， 
			
			Exchanger（交换者）是一个用于线程间协作的工具类。Exchanger用于进行线程间的数据交换。
				它提供一个同步点，在这个同步点，两个线程可以交换彼此的数据。
				这两个线程通过exchange方法交换数据，如果第一个线程先执行exchange()方法，它会一直等待第二个线程也执行exchange方法，
				当两个线程都到达同步点时，这两个线程就可以交换数据，将本线程生产出来的数据传递给对方。
			
		}
		
		Callable、Future和FutureTask ：{
			isDone，结束，正常还是异常结束，或者自己取消，返回true；
			isCancelled 任务完成前被取消，返回true；
			cancel（boolean）：
			1、	任务还没开始，返回false
			2、	任务已经启动，cancel（true），中断正在运行的任务，中断成功，返回true，cancel（false），不会去中断已经运行的任务
			3、	任务已经结束，返回false

			包含图片和文字的文档的处理：图片（云上），可以用future去取图片，主线程继续解析文字。

			Runnable是一个接口，在它里面只声明了一个run()方法，由于run()方法返回值为void类型，所以在执行完任务之后无法返回任何结果。
			Callable位于java.util.concurrent包下，它也是一个接口，在它里面也只声明了一个方法，只不过这个方法叫做call()，这是一个泛型接口，call()函数返回的类型就是传递进来的V类型。
			Future就是对于具体的Runnable或者Callable任务的执行结果进行取消、查询是否完成、获取结果。必要时可以通过get方法获取执行结果，该方法会阻塞直到任务返回结果。
			
	
		}
	
}



//还有一个作业没做，
7/原子操作CAS（Compare And Swap）:{
	
	底层实现：CAS指令
		乐观锁
		悲观锁--->控制不好，会发生死锁，
		JDK CAS机制--->无锁化编程
		数据库的操作在JDK里是原子操作；
		加锁是一个悲观锁，
		事务是一个典型的悲观锁
		synchronized比较重，
		0.6纳秒：cpu执行一条指令，
		
	
	CAS（Compare And Swap）：比较并且交换-->两个动作，这两个动作合起来就是一个原子操作，  //JDK1.8大量使用了循环CAS，
	线程采用CAS机制，本质是不会抢夺锁的，提供了一种乐观锁机制，
		synchronized是原子操作，
	现代处理器保证处理，提供CAS指令，
		自旋是死循环，
		使用CAS机制达到原子操作的目的
	
	原理：compare内存中变量和旧值比较，如果相等，旧值swap为新值，如果不相等，再来一次，get变量值(旧值)--->计算后得到新值，把新值和旧值继续比较，如果相等，旧值swap为新值...
	
	什么是原子操作？如何实现原子操作？{
		假定有两个操作A和B，如果从执行A的线程来看，当另一个线程执行B时，要么将B全部执行完，要么完全不执行B，那么A和B对彼此来说是原子的。
		实现原子操作可以使用锁，锁机制，满足基本的需求是没有问题的了，但是有的时候我们的需求并非这么简单，我们需要更有效，更加灵活的机制，
			synchronized关键字是基于阻塞的锁机制，也就是说当一个线程拥有锁的时候，访问同一资源的其它线程需要等待，直到该线程释放锁，
		这里会有些问题：首先，如果被阻塞的线程优先级很高很重要怎么办？其次，如果获得锁的线程一直不释放锁怎么办？（这种情况是非常糟糕的）。
			还有一种情况，如果有大量的线程来竞争资源，那CPU将会花费大量的时间和资源来处理这些竞争，同时，还有可能出现一些例如死锁之类的情况，
			最后，其实锁机制是一种比较粗糙，粒度比较大的机制，相对于像计数器这样的需求有点儿过于笨重。
		实现原子操作还可以使用当前的处理器基本都支持CAS()的指令，只不过每个厂家所实现的算法并不一样，每一个CAS操作过程都包含三个运算符：
			一个内存地址V，一个期望的值A和一个新值B，操作的时候如果这个地址上存放的值等于这个期望的值A，则将地址上的值赋为新值B，否则不做任何操作。
		CAS的基本思路就是，如果这个地址上的值和期望的值相等，则给其赋予新值，否则不做任何事儿，但是要返回原值是多少。循环CAS就是在一个循环里不断的做cas操作，直到成功为止。
		CAS是怎么实现线程的安全呢？语言层面不做处理，我们将其交给硬件—CPU和内存，利用CPU的多处理能力，实现硬件层面的阻塞，再加上volatile变量的特性即可实现基于原子操作的线程安全。

	}
	
	CAS问题：
		1/ABA问题：版本戳，AtomicStampedReference(只关心变化的次数,有一个计数器)/AtomicMarkableReference(只关心有没有变化);
			底层：现代处理器的内部处理机制，CAS指令.{
				因为CAS需要在操作值的时候，检查值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，
					那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。
				ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加1，那么A→B→A就会变成1A→2B→3A。举个通俗点的例子，你倒了一杯水放桌子上，
					干了点别的事，然后同事把你水喝了又给你重新倒了一杯水，你回来看水还在，拿起来就喝，如果你不管水中间被人喝过，只关心水还在，这就是ABA问题。
				如果你是一个讲卫生讲文明的小伙子，不但关心水在不在，还要在你离开的时候水被人动过没有，因为你是程序员，所以就想起了放了张纸在旁边，写上初始值0，
					别人喝水前麻烦先做个累加才能喝水。

			}
		
		2/开销问题：一个CAS指令对应一个内存地址;{
			自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。
		}
		
		3/只能保证一个共享变量的原子操作，{
			当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁。
			还有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如，有两个共享变量i＝2，j=a，合并一下ij=2a，然后用CAS来操作ij。
				从Java 1.5开始，JDK提供了AtomicReference类来保证引用对象之间的原子性，就可以把多个变量放在一个对象里来进行CAS操作。

		}
		
	
	
	Jdk中相关原子操作类的使用：{
		
		更新基本类型--->AtomicInteger ：{
			•int addAndGet（int delta）：以原子方式将输入的数值与实例中的值（AtomicInteger里的value）相加，并返回结果。
			•boolean compareAndSet（int expect，int update）：如果输入的数值等于预期值，则以原子方式将该值设置为输入的值。
			•int getAndIncrement()：以原子方式将当前值加1，注意，这里返回的是自增前的值。
			•int getAndSet（int newValue）：以原子方式设置为newValue的值，并返回旧值。
		}
		
		更新数组类--->AtomicIntegerArray:{
			主要是提供原子的方式更新数组里的整型，其常用方法如下。
			•int addAndGet（int i，int delta）：以原子方式将输入值与数组中索引i的元素相加。
			•boolean compareAndSet（int i，int expect，int update）：如果当前值等于预期值，则以原子方式将数组位置i的元素设置成update值。
			需要注意的是，数组value通过构造方法传递进去，然后AtomicIntegerArray会将当前数组复制一份，所以当AtomicIntegerArray对内部的数组元素进行修改时，不会影响传入的数组。
		}
		
		更新引用类型：{
			原子更新基本类型的AtomicInteger，只能更新一个变量，如果要原子更新多个变量，就需要使用这个原子更新引用类型提供的类。Atomic包提供了以下3个类。
			AtomicReference
				原子更新引用类型。
			AtomicStampedReference：变化的次数
				利用版本戳的形式记录了每次改变以后的版本号，这样的话就不会存在ABA问题了。这就是AtomicStampedReference的解决方案。AtomicMarkableReference跟AtomicStampedReference差不多， AtomicStampedReference是使用pair的int stamp作为计数器使用，AtomicMarkableReference的pair使用的是boolean mark。 还是那个水的例子，AtomicStampedReference可能关心的是动过几次，AtomicMarkableReference关心的是有没有被人动过，方法都比较简单。
			AtomicMarkableReference：只关心有没有变化
				原子更新带有标记位的引用类型。可以原子更新一个布尔类型的标记位和引用类型。构造方法是AtomicMarkableReference（V initialRef，booleaninitialMark）。

		}	
		
		原子更新字段类：{
			如果需原子地更新某个类里的某个字段时，就需要使用原子更新字段类，Atomic包提供了以下3个类进行原子字段更新。
			要想原子地更新字段类需要两步。第一步，因为原子更新字段类都是抽象类，每次使用的时候必须使用静态方法newUpdater()创建一个更新器，并且需要设置想要更新的类和属性。第二步，更新类的字段（属性）必须使用public volatile修饰符。
			AtomicIntegerFieldUpdater：
				原子更新整型的字段的更新器。
			AtomicLongFieldUpdater：
				原子更新长整型字段的更新器。
			AtomicReferenceFieldUpdater：
				原子更新引用类型里的字段。
		}
		
		
	}
	
		
		
}



自旋是死循环，for(;;)
唤醒一个线程需要进行上下文切换，花费：5000~10000个时钟周期；
当一个线程被终止，它所持有的所有资源都会被释放，
8/显示锁-lock: {
	
	没有特殊情况尽量使用synchronized,
	把线程打包成一个节点，myPred域，locked域标志线程状态，
	
	Lock是显示锁，手动加锁去锁；JDK语法实现，
	synchronized内置锁，java语言特性实现的，语言特性决定的；是非公平锁；独占锁（悲观锁，做任何事先拿到锁再说）；
	可重入锁ReentrantLock：独占锁；
	
	如果没有使用tryLock和lockInterruptibly中断锁，还是使用synchronized比较好，资源消耗要少一些；
	synchronized的使用比显示锁资源消耗要少一些，因为Lock是一个类，用的时候需要new一个实例，增加内存开销
	
	显示锁的标准范式,用try/finally包着使用：{
		lock.lock();
		try{
			age--;
		} finally {
			lock.unlock();
		}
	}
	
	可重入锁ReentrantLock：递归（一个线程在持有这把锁的时候，下次仍然会获得这把锁）{
		synchronized在实现的时候考虑到了可重入锁的情况，ReentrantLock也考虑到了；
	}
	
	锁的可重入：ReentrantLock{
		简单地讲就是：“同一个线程对于已经获得到的锁，可以多次继续申请到该锁的使用权”。
		而synchronized关键字隐式的支持重进入，比如一个synchronized修饰的递归方法，在方法执行时，执行线程在获取了锁之后仍能连续多次地获得该锁。
		ReentrantLock在调用lock()方法时，已经获取到锁的线程，能够再次调用lock()方法获取锁而不被阻塞。
	}
	
	
	面试题：
		可重入锁ReentrantLock、所谓锁的公平和非公平：{
			
			锁的可重入：ReentrantLock如上所示。
			
			公平和非公平锁---->非公平锁比公平锁效率高：{
				如果在时间上，先对锁进行获取的请求一定先被满足，那么这个锁是公平的，反之，是不公平的。
					公平的获取锁，也就是等待时间最长的线程最优先获取锁，也可以说锁获取是顺序的。 ReentrantLock提供了一个构造函数，能够控制锁是否是公平的。
					事实上，公平的锁机制往往没有非公平的效率高。  
				在激烈竞争的情况下,非公平锁的性能高于公平锁的性能的一个原因是:在恢复一个被挂起的线程与该线程真正开始运行之间存在着严重的延迟。
					假设线程A持有一个锁,并且线程B请求这个锁。由于这个锁已被线程A持有,因此B将被挂起。当A释放锁时,B将被唤醒,因此会再次尝试获取锁。
					与此同时,如果C也请求这个锁,那么C很可能会在B被完全唤醒之前获得、使用以及释放这个锁。
					这样的情况是一种“双赢”的局面:B获得锁的时刻并没有推迟,C更早地获得了锁,并且吞吐量也获得了提高。
				唤醒一个线程需要进行上下文切换，花费：5000~10000个时钟周期；
			}
			
		}
	
	
	//数据库读多写少，读写分离，读写比例是10：1的场景，	
	读锁和写锁是互斥的：
		读线程排斥其他写线程：读线程拿到读锁，写线程想拿写锁是不允许的，直到读线程释放读锁才允许写线程拿写锁进行操作；
		读锁不排斥读锁，但排斥写锁；而写锁排斥读写锁；
		写独占，读共享；
	读写锁ReentrantReadWriteLock：{
		之前提到锁（如Mutex和ReentrantLock）基本都是排他锁，这些锁在同一时刻只允许一个线程进行访问，而读写锁在同一时刻可以允许多个读线程访问，
			但是在写线程访问时，所有的读线程和其他写线程均被阻塞。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排他锁有了很大提升。
		除了保证写操作对读操作的可见性以及并发性的提升之外，读写锁能够简化读写交互场景的编程方式。假设在程序中定义一个共享的用作缓存数据结构，
			它大部分时间提供读服务（例如查询和搜索），而写操作占有的时间很少，但是写操作完成之后的更新需要对后续的读服务可见。
		在没有读写锁支持的（Java 5之前）时候，如果需要完成上述工作就要使用Java的等待通知机制，就是当写操作开始时，所有晚于写操作的读操作均会进入等待状态，
			只有写操作完成并进行通知之后，所有等待的读操作才能继续执行（写操作之间依靠synchronized关键进行同步），这样做的目的是使读操作能读取到正确的数据，不会出现脏读。
			改用读写锁实现上述功能，只需要在读操作时获取读锁，写操作时获取写锁即可。当写锁被获取到时，后续（非当前写操作线程）的读写操作都会被阻塞，写锁释放之后，
			所有操作继续执行，编程方式相对于使用等待通知机制的实现方式而言，变得简单明了。
		一般情况下，读写锁的性能都会比排它锁好，因为大多数场景读是多于写的。在读多于写的情况下，读写锁能够提供比排它锁更好的并发性和吞吐量
		ReentrantReadWriteLock其实实现的是ReadWriteLock接口。
	}
			

	Condition接口：{
		任意一个Java对象，都拥有一组监视器方法（定义在java.lang.Object上），主要包括wait()、wait(long timeout)、notify()以及notifyAll()方法，
			这些方法与synchronized同步关键字配合，可以实现等待/通知模式。Condition接口也提供了类似Object的监视器方法，与Lock配合可以实现等待/通知模式。
	}
	
	
	
	
	
	锁的重入：一个线程拿到一个锁之后，可以再次拿到这个锁，
	

	
	synchronized:内置锁,由java语言特性决定的，获取锁，但是把synchronized固化了，只能先获取后释放，
	synchronized：获取锁是不能中断的，只要没有wait就会一直获取下去，也没有超时的说法，只要获取不到就会一直获取下去，
	

	
	Lock接口和synchronized比较：
		synchronized 代码简洁，Lock：获取锁可以被中断，超时获取锁，尝试获取锁，读多写少用读
		
	可重入锁ReentrantLock、所谓锁 的公平和非公平
		非公平锁的效率更高--->把线程从恢复挂起到，从挂起中恢复到线程真正运行起来，这段时间充分利用起来了
			非公平锁允许插队，
			
	ReadWriteLock接口和读写锁ReentrantReadWriteLock
		ReentrantLock和Synchronized关键字，都是排他锁，同一时刻只允许一个读线程同时访问
		读写锁：同一时刻允许多个读线程同时访问，但是写线程访问的时候，所有的读和写都被阻塞，最适宜与读多写少的情况
		ReentrantReadWriteLock在内部维护了一个读锁一个写锁，
		
		读写锁ReentrantReadWriteLock:读多写少考虑用读写锁， 
		
}		

	
//该看AQS了，	
9/AQS:{
	
	AQS是CLH队列锁的变种，
	AQS实现锁的一种方式，一个同步组件，一种思想，AQS和锁没有关系，
	
	了解LockSupport：{
		
		LockSupport工具：	
			作用：
				唤醒一个线程
				阻塞一个线程
			park开头的方法
				负责阻塞线程
			unpark(Thread thread)方法
				负责唤醒线程
				
		LockSupport定义了一组的公共静态方法，这些方法提供了最基本的线程阻塞和唤醒功能，而LockSupport也成为构建同步组件的基础工具。
		LockSupport定义了一组以park开头的方法用来阻塞当前线程，以及unpark(Thread thread)方法来唤醒一个被阻塞的线程。
			LockSupport增加了park(Object blocker)、parkNanos(Object blocker,long nanos)和parkUntil(Object blocker,long deadline)3个方法，
			用于实现阻塞当前线程的功能，其中参数blocker是用来标识当前线程在等待的对象（以下称为阻塞对象），该对象主要用于问题排查和系统监控。

	}
	
	//是一个单向链表
	CLH队列锁：{
		我们使用的pc机服务器里面都是CLH队列锁的实现，基于链表的锁的实现，
		多个线程获得同一个锁，先把自己声明成一个节点，
		
		CLH队列锁即Craig, Landin, and Hagersten (CLH) locks。
		CLH队列锁也是一种基于链表的可扩展、高性能、公平的自旋锁，申请线程仅仅在本地变量上自旋，它不断轮询前驱的状态，假设发现前驱释放了锁就结束自旋。
		当一个线程需要获取锁时：
		1/创建一个的QNode，将其中的locked设置为true表示需要获取锁，myPred表示对其前驱结点的引用
		2/线程A对tail域调用getAndSet方法，使自己成为队列的尾部，同时获取一个指向其前驱结点的引用myPred
				线程B需要获得锁，同样的流程再来一遍
		3/线程就在前驱结点的locked字段上旋转，直到前驱结点释放锁(前驱节点的锁值 locked == false)
		4/当一个线程需要释放锁时，将当前结点的locked域设置为false，同时回收前驱结点
		如上图所示，前驱结点释放锁，线程A的myPred所指向的前驱结点的locked字段变为false，线程A就可以获取到锁。
		CLH队列锁的优点是空间复杂度低（如果有n个线程，L个锁，每个线程每次只获取一个锁，那么需要的存储空间是O（L+n），n个线程有n个myNode，L个锁有L个tail）。
			CLH队列锁常用在SMP体系结构下。
		Java中的AQS是CLH队列锁的一种变体实现。
		
	}
	
	学习AQS的必要性：{
		队列同步器AbstractQueuedSynchronizer（以下简称同步器或AQS），是用来构建锁或者其他同步组件的基础框架，它使用了一个int成员变量表示同步状态，
			通过内置的FIFO队列来完成资源获取线程的排队工作。并发包的大师（Doug Lea）期望它能够成为实现大部分同步需求的基础。
	}
	
	AQS使用方式和其中的设计模式：{
		AQS的主要使用方式是继承，子类通过继承AQS并实现它的抽象方法来管理同步状态，在AQS里由一个int型的state来代表这个状态，在抽象方法的实现过程中免不了要对同步状态进行更改，
			这时就需要使用同步器提供的3个方法（getState()、setState(int newState)和compareAndSetState(int expect,int update)）来进行操作，因为它们能够保证状态的改变是安全的。
		在实现上，子类推荐被定义为自定义同步组件的静态内部类，AQS自身没有实现任何同步接口，它仅仅是定义了若干同步状态获取和释放的方法来供自定义同步组件使用，
			同步器既可以支持独占式地获取同步状态，也可以支持共享式地获取同步状态，这样就可以方便实现不同类型的同步组件（ReentrantLock、ReentrantReadWriteLock和CountDownLatch等）。
		同步器是实现锁（也可以是任意同步组件）的关键，在锁的实现中聚合同步器。可以这样理解二者之间的关系：
		锁是面向使用者的，它定义了使用者与锁交互的接口（比如可以允许两个线程并行访问），隐藏了实现细节；
		同步器面向的是锁的实现者，它简化了锁的实现方式，屏蔽了同步状态管理、线程的排队、等待与唤醒等底层操作。锁和同步器很好地隔离了使用者和实现者所需关注的领域。
		实现者需要继承同步器并重写指定的方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模板方法，而这些模板方法将会调用使用者重写的方法。

	}
	
	模板方法模式：{
		同步器的设计基于模板方法模式。模板方法模式的意图是，定义一个操作中的算法的骨架，而将一些步骤的实现延迟到子类中。
			模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。我们最常见的就是Spring框架里的各种Template。
			
		
	}
	
	AQS中的方法：{
		
		模板方法：{
			实现自定义同步组件时，将会调用同步器提供的模板方法，
			这些模板方法同步器提供的模板方法基本上分为3类：独占式获取与释放同步状态、共享式获取与释放、同步状态和查询同步队列中的等待线程情况。
		}
		
		可重写的方法：{
			
		}
		
		访问或修改同步状态的方法：{
			重写同步器指定的方法时，需要使用同步器提供的如下3个方法来访问或修改同步状态。
				•getState()：获取当前同步状态。
				•setState(int newState)：设置当前同步状态。
				•compareAndSetState(int expect,int update)：使用CAS设置当前状态，该方法能够保证状态设置的原子性。 

		}
		
	}
	
	实现一个自己的独占锁
	
	深入源码：
		AQS中的数据结构-节点和同步队列：{
			
			节点Node：
				既然说Java中的AQS是CLH队列锁的一种变体实现，毫无疑问，作为队列来说，必然要有一个节点的数据结构来保存我们前面所说的各种域，比如前驱节点，节点的状态等，
					这个数据结构就是AQS中的内部类Node。作为这个数据结构应该关心些什么信息？
				1/线程信息，肯定要知道我是哪个线程；
				2/队列中线程状态，既然知道是哪一个线程，肯定还要知道线程当前处在什么状态，是已经取消了“获锁”请求，还是在“”等待中”，或者说“即将得到锁”
				3/前驱和后继线程，因为是一个等待队列，那么也就需要知道当前线程前面的是哪个线程，当前线程后面的是哪个线程（因为当前线程释放锁以后，理当立马通知后继线程去获取锁）。
				其中包括了：
					线程的2种等待模式：
						SHARED：表示线程以共享的模式等待锁（如ReadLock）
						EXCLUSIVE：表示线程以互斥的模式等待锁（如ReetrantLock），互斥就是一把锁只能由一个线程持有，不能同时存在多个线程使用同一个锁
					线程在队列中的状态枚举：
						CANCELLED：值为1，表示线程的获锁请求已经“取消”
						SIGNAL：值为-1，表示该线程一切都准备好了,就等待锁空闲出来给我
						CONDITION：值为-2，表示线程等待某一个条件（Condition）被满足
						PROPAGATE：值为-3，当线程处在“SHARED”模式时，该字段才会被使用上
						初始化Node对象时，默认为0
					成员变量：
						waitStatus：该int变量表示线程在队列中的状态，其值就是上述提到的CANCELLED、SIGNAL、CONDITION、PROPAGATE
						prev：该变量类型为Node对象，表示该节点的前一个Node节点（前驱）
						next：该变量类型为Node对象，表示该节点的后一个Node节点（后继）
						thread：该变量类型为Thread对象，表示该节点的代表的线程
						nextWaiter：该变量类型为Node对象，表示等待condition条件的Node节点
					当前线程获取同步状态失败时，同步器会将当前线程以及等待状态等信息构造成为一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，
						会把首节点中的线程唤醒，使其再次尝试获取同步状态。同步队列中的节点（Node）用来保存获取同步状态失败的线程引用、等待状态以及前驱和后继节点。

			head和tail：
				AQS还拥有首节点（head）和尾节点（tail）两个引用，一个指向队列头节点，而另一个指向队列尾节点。
				注意：因为首节点head是不保存线程信息的节点，仅仅是因为数据结构设计上的需要，在数据结构上，这种做法往往叫做“空头节点链表”。
					对应的就有“非空头结点链表”

		}

	
	AbstractQueuedSynchronizer深入分析：---Doug Lee;
			了解AQS，就了解jdk并发包的半壁江山
	什么是AQS？学习它的必要性
		AQS使用方式和其中的设计模式
			继承，模板方法设计模式
		了解其中的方法
			模板方法：
				独占式获取
					accquire
					acquireInterruptibly
					tryAcquireNanos
				共享式获取
					acquireShared
					acquireSharedInterruptibly
					tryAcquireSharedNanos
				独占式释放锁
					release
				共享式释放锁
					releaseShared
			需要子类覆盖的流程方法
				独占式获取  tryAcquire
				独占式释放  tryRelease
				共享式获取 tryAcquireShared
				共享式释放  tryReleaseShared
				这个同步器是否处于独占模式  isHeldExclusively

			同步状态state：
			getState:获取当前的同步状态
			setState：设置当前同步状态
			compareAndSetState 使用CAS设置状态，保证状态设置的原子性
		

		AQS中的数据结构-节点和同步队列
			竞争失败的线程会打包成Node放到同步队列，Node可能的状态里：
			CANCELLED：线程等待超时或者被中断了，需要从队列中移走
			SIGNAL：后续的节点等待状态，当前节点，通知后面的节点去运行
			CONDITION :当前节点处于等待队列
			PROPAGATE：共享，表示状态要往后面的节点传播
			0，	表示初始状态

			AbstractQueuedSynchronizer：{
				static final class Node {
				/** Marker to indicate a node is waiting in shared mode */
				static final Node SHARED = new Node();
				/** Marker to indicate a node is waiting in exclusive mode */
				static final Node EXCLUSIVE = null;

				/** waitStatus value to indicate thread has cancelled */
				static final int CANCELLED =  1;
				/** waitStatus value to indicate successor's thread needs unparking */
				static final int SIGNAL    = -1;
				/** waitStatus value to indicate thread is waiting on condition */
				static final int CONDITION = -2;
				/**
				 * waitStatus value to indicate the next acquireShared should
				 * unconditionally propagate
				 */
				static final int PROPAGATE = -3;
				
				volatile int waitStatus;

				volatile Node prev;

				volatile Node next;
			   
				volatile Thread thread;

				Node nextWaiter;


			}
			}
			
			CAS自旋，不断尝试去拿锁，{
				private Node enq(final Node node) {
					for (;;) {
						Node t = tail;
						if (t == null) { // Must initialize
							if (compareAndSetHead(new Node()))
								tail = head;
						} else {
							node.prev = t;
							if (compareAndSetTail(t, node)) {
								t.next = node;
								return t;
							}
						}
					}
				}
			}
			
		独占式同步状态获取与释放
		
		其他同步状态获取与释放 
			共享锁同步状态获取与释放
			独占式超时同步状态获取
			再次实战，实现一个奇葩点的三元共享同步工具类，
		
		Condition分析
		
		总结：
			读写锁的升级和降级：
				写锁可以降级为读锁，但读锁不能升级为写锁，
			自旋锁和排他锁，
			共享锁和独占锁
			
		
}



java中的各种容器list/map
10/并发容器：{
	
	Hash:{
		散列：意译；哈希：音译----->一般翻译为“散列”，音译为“哈希”的，
		
		把任意长度的输入(预映射)通过散列算法，变换成为固定长度的输出，这个输出值就是散列值。这种转换是一种压缩映射，属于压缩映射。
			也就是说散列的空间值远小于输入空间值，不同的输入可能造成相同的输出；所以不可能从散列值来唯一确定输入值，容易产生哈希冲突。
			简单的说就是一种将任意长度的消息压缩到某一固定长度的消息摘要的函数。
			Hash算法有直接取余法、乘法取整法、平方取中法等。
			产生哈希冲突时解决办法：
				1/开放寻址法:如果发生冲突，再继续往后找直到找出不冲突为止；
				2/再散列：用hash算法在散列一次；就是对具有相同hash值得数再求一次hash值进行比较；
				3/链地址法（相同hash值的元素用链表串起来）。
			md4、md5、sha-hash(不能笼统得称为加密算法):摘要算法，签名算法，哈希算法的一种，都是不可逆的，
			md4,md5,sha-hash算法也属于hash算法，又称摘要算法。
			md5可以破解：其实不是破解，使用彩虹表匹配----->撞库：通过密文反推明文，用彩虹表和密文进行比较，一样就认为找出了明文
				md5一般不会直接对明文进行加密，先对明文加盐，
			ConcurrentHashMap在发生hash冲突时采用了链地址法。
	}
			
	
	位运算：{
		2的0次方 = 1，2的1次方=2…….，以上表格代表数字 （2的5次方+2的3次方）= 40
		由上面的表格可以看出，数字类型在数字渐渐变大时，是由低位慢慢向高位扩展的。

		Java实际保存int型时 正数  第31位 =0 	负数：第31位=1
		常用位运算有：
			位与  &  (1&1=1 	1&0=0	 0&0=0)                //两个都是1才是1
			位或  |   (1|1=1		 1|0=1 	0|0=0)			   //只要有一个是1就是1
			位非  ~  （ ~1=0 	 ~0=1）						   //取反，取否定
			位异或  ^   (1^1=0	 1^0=1	 0^0=0) 			   //两个数一样是0，只有不一样的情况下才是1，
			<<有符号左移     //若正数,高位补0；负数,高位补1
			>>有符号的右移    
			>>>无符号右移    //不论正负,高位均补0
				例如：8 << 2 = 32	8>>2 = 2
				正数高位补0，负数高位补1，
			取模的操作 a % (Math.pow(2,n)) 等价于 a&( Math.pow(2,n)-1)
					   a % (2^n) 等价于a & (2^n-1) 
			有趣的取模性质：取模a % (2^n) 等价于 a & (2^n - 1)，所以在map里的数组个数一定是2的乘方数，计算key值在哪个元素中的时候，就用位运算来快速定位。
		
		哪里可以用到位运算：
			Java中的类修饰符、成员变量修饰符、方法修饰符
			Java容器中的HashMap和ConcurrentHashMap的实现
			权限控制或者商品属性
			简单可逆加密（位异或可用于简单的加密操作）
		实战：将位运算用在权限控制、商品属性上
			节省很多代码量
			效率高
			属性变动影响小
			不直观
		案例：
			位运算适用：权限控制，物品的属性非常多时的保存
			位运算取代取模运算，速度要快，
	}
	
	int32位，
	如何快速判定一个数是奇数偶数？利用位运算进行。
	
	
		
	我们知道HashMap是线程不安全的，在多线程环境下，使用Hashmap进行put操作会引起死循环，导致CPU利用率接近100%，所以在并发情况下不能使用HashMap。	
	
	HashMap在1.7/中的死循环分析：{
		这种死循环是在HashMap扩容里面造成的，
		Hashmap多线程会导致HashMap的Entry链表形成环形数据结构，一旦形成环形数据结构，Entry的next节点永远不为空，就会产生死循环获取Entry。
		put：{
				public V put(K key, V value) {
					//如果table数组为空数组{}，进行数组填充（为table分配实际内存空间），入参为threshold，此时threshold为initialCapacity 默认是1<<4(2 4=16)
					if (table == EMPTY_TABLE) {
						inflateTable(threshold);
					}
					// HashMap允许存放null键和null值。
					// 当key为null时，调用putForNullKey方法，将value放置在数组第一个位置。
					if (key == null)
						return putForNullKey(value);
					// 根据key的keyCode重新计算hash值。
					int hash = hash(key.hashCode());
					// 搜索指定hash值在对应table中的索引。
					int i = indexFor(hash, table.length);
					// 如果 i 索引处的 Entry 不为 null，通过循环不断遍历 e 元素的下一个元素。
					for (Entry<K,V> e = table[i]; e != null; e = e.next) {
						Object k;
						if (e.hash == hash && ((k = e.key) == key || key.equals(k))) {
							// 如果发现已有该键值，则存储新的值，并返回原始值
							 V oldValue = e.value;
							 e.value = value;
							 e.recordAccess(this);
							 return oldValue;
						}
					 }
					 // 如果i索引处的Entry为null，表明此处还没有Entry。
					 modCount++;
					 // 将key、value添加到i索引处。
					 addEntry(hash, key, value, i);
					 return null;
				}
			
		}
		
		addEntry：{
			public void addEntry(int hash, K key, V value, int bucketIndex) {
				if ((size >= threshold) && (null != table[bucketIndex])) {
					resize(2 * table.length);//当size超过临界阈值threshold，并且即将发生哈希冲突时进行扩容
					hash = (null != key) ? hash(key) : 0;
					bucketIndex = indexFor(hash, table.length);
				}

				createEntry(hash, key, value, bucketIndex);
			}
		}
		
		resize:{
			public void resize(int newCapacity) {
				Entry[] oldTable = table;
				int oldCapacity = oldTable.length;
				if (oldCapacity == MAXIMUM_CAPACITY) {
					threshold = Integer.MAX_VALUE;
					return;
				}

				Entry[] newTable = new Entry[newCapacity];
				transfer(newTable, initHashSeedAsNeeded(newCapacity));
				table = newTable;
				threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);
			}
		}
		
		//单线程下
		在1.7/中HashMap发生hash冲突时：插入元素是插在头部，
		transfer：{
			public void transfer(Entry[] newTable, boolean rehash) {
				int newCapacity = newTable.length;
		　　　　　//for循环中的代码，逐个遍历链表，重新计算索引位置，将老数组数据复制到新数组中去（数组不存储实际数据，所以仅仅是拷贝引用而已）
				//循环----------------------------------------------->头插法，
				for (Entry<K,V> e : table) {     
					while(null != e) {
						Entry<K,V> next = e.next;
						if (rehash) {
							e.hash = null == e.key ? 0 : hash(e.key);
						}
						int i = indexFor(e.hash, newCapacity);
		　　　　　　　　　 //将当前entry的next链指向新的索引位置,newTable[i]有可能为空，有可能也是个entry链，如果是entry链，直接在链表头部插入。
						e.next = newTable[i];//循环-------------------->头插法，
						newTable[i] = e;	//循环-------------------->头插法，
						e = next;			//循环-------------------->头插法，
					}
				}								
				//循环----------------------------------------------->头插法，
			}
		}
		
		//多线程下
		在1.7/中HashMap发生hash冲突时：插入元素是插在头部，
		transfer：{
			public void transfer(Entry[] newTable, boolean rehash) {
				int newCapacity = newTable.length;
		　　　　　//for循环中的代码，逐个遍历链表，重新计算索引位置，将老数组数据复制到新数组中去（数组不存储实际数据，所以仅仅是拷贝引用而已）
				//循环----------------------------------------------->头插法，
				for (Entry<K,V> e : table) {     
					while(null != e) {
						Entry<K,V> next = e.next;
						if (rehash) {
							e.hash = null == e.key ? 0 : hash(e.key);
						}
						int i = indexFor(e.hash, newCapacity);
		　　　　　　　　　 //将当前entry的next链指向新的索引位置,newTable[i]有可能为空，有可能也是个entry链，如果是entry链，直接在链表头部插入。
						e.next = newTable[i];//循环-------------------->头插法，
						newTable[i] = e;	//循环-------------------->头插法，
						e = next;			//循环-------------------->头插法，
					}
				}								
				//循环----------------------------------------------->头插法，
			}
		}
		
		
		为什会产生死循环：//产生一个循环链表：
			就是因为在HashMap的1.7/有两个线程进行并发扩容的时候，一个线程完成了全部的扩容操作后，而另外一份线程在执行过程中被挂起，被挂起的线程在恢复执行以后，
				在自己的内部new table里面重新扩容操作，即头插法链表操作：造成已有的元素(next引用)的指针相互指着，形成一个循环链表，就造成了死循环。
			面试时有可能让面试者画出死循环过程。
		死循环总结：
			HashMap之所以在并发下的扩容造成死循环，是因为，多个线程并发进行时，因为一个线程先期完成了扩容，将原Map的链表重新散列到自己的表中，
				并且链表变成了倒序，后一个线程再扩容时，又进行自己的散列，再次将倒序链表变为正序链表。于是形成了一个环形链表，当get表中不存在的元素时，造成死循环。

			
	}
		
	HashMap在1.8/中的不排除在并发情况下出现线程安全的情况下，至少目前我没遇到过死循环的情况：{
		
	}
		
		
		
	ConcurrentHashMap；
		hashmap有的方法，ConcurrentHashMap都有，
	
	ConcurrentHashMap本身是线程安全的，保证调用它的方法的时候是线程安全的。	
	除了Map系列应该有的线程安全的get，put等方法外，ConcurrentHashMap还提供了一个在并发下比较有用的方法 putIfAbsent

	1.7/和1.8/中的ConcurrentHashMap的异同：
		不同：数据结构和加锁方面不同；
		相同：本质都是分段锁。
	ConcurrentHashMap在1.7/中的实现：{
		
		ConcurrentHashMap持有Segment数组(Segment<K,V>[] segments)，Segment中的每一个元素持有(HashEntry<K,V>[] table)
		
		总结：
			ConcurrentHashMap：1.7/弱一致的，因为get没有加锁，
			分段锁称为Segment，Segmen继承锁，本身就是一个锁，是一种可重入锁，Segmen是一个中间数据结构；
			Segment允许线程访问的并发度(允许多少个线程同时访问Segment)，初始化容量16，后期也不能扩容了。
			ensureSegmen初始化其中一个Segmen槽，
			为什么是2的n次幂？----->快速定位、减少数据迁移量、缓存；
			
			尽量不要用size(),
				因为size()对Segmen加锁，影响性能，用isEmpty()判断容量;
				containsValue()同理，也是加锁影响性能。
	
			为什么扩容？----->减少碰撞的几率、数据散列均匀；
		
		初始化：
			Segment和table都是2的N次方，
			Segment有16个元素，Segment数组初始化后不可扩容，
				Segment初始化后只有Segment[0]下的数组才会挂table数组，其他的Segment[i]在put时才会挂上数组，
			initialCapacity和loadFactor初始化table大小，
			
			不是所有table扩容，哪一个需要扩容就扩容
			
		get：取值需要两次定位，用的是同一个hash值：{
			先定位到Segment(hash值高位取值定位)，再定位到table(取全部hash值进行定位)；
			
			public V get(Object key) {
				Segment<K,V> s; // manually integrate access methods to reduce overhead
				HashEntry<K,V>[] tab;
				int h = hash(key);
				long u = (((h >>> segmentShift) & segmentMask) << SSHIFT) + SBASE;
				if ((s = (Segment<K,V>)UNSAFE.getObjectVolatile(segments, u)) != null &&
					(tab = s.table) != null) {
					for (HashEntry<K,V> e = (HashEntry<K,V>) UNSAFE.getObjectVolatile
							 (tab, ((long)(((tab.length - 1) & h)) << TSHIFT) + TBASE);
						 e != null; e = e.next) {
						K k;
						if ((k = e.key) == key || (e.hash == h && key.equals(k)))
							return e.value;
					}
				}
				return null;
			}
			
			get没有做加锁，怎么保证拿到的最新值：{
				//HashEntry类
				static final class HashEntry<K,V> {
						final int hash;
						final K key;
						volatile V value;
						volatile HashEntry<K,V> next;
				}
				final定位常数
				volatile同步，get取值要么是不变的，要么是最新值，
			}
				
			
		}
		
				
		put：{
			
			这个put方法在Segment里，
			final V put(K key, int hash, V value, boolean onlyIfAbsent) {
				HashEntry<K,V> node = tryLock() ? null :
					scanAndLockForPut(key, hash, value);//自旋拿锁，
				V oldValue;
				try {
					HashEntry<K,V>[] tab = table;
					int index = (tab.length - 1) & hash;
					HashEntry<K,V> first = entryAt(tab, index);
					for (HashEntry<K,V> e = first;;) {
						if (e != null) {
							K k;
							if ((k = e.key) == key ||
								(e.hash == hash && key.equals(k))) {
								oldValue = e.value;
								if (!onlyIfAbsent) {
									e.value = value;
									++modCount;
								}
								break;
							}
							e = e.next;
						}
						else {
							if (node != null)
								node.setNext(first);//1.7里面用的是头插法，
							else
								node = new HashEntry<K,V>(hash, key, value, first);
							int c = count + 1;
							if (c > threshold && tab.length < MAXIMUM_CAPACITY)
								rehash(node);//rehash扩容机制，不用加锁，它已经加锁了，
							else
								setEntryAt(tab, index, node);
							++modCount;
							count = c;
							oldValue = null;
							break;
						}
					}
				} finally {
					unlock();
				}
				return oldValue;
			}
			
			
			
		}
		
		
		//扩容方法
		rehash：{
			
			private void rehash(HashEntry<K,V> node) {
				HashEntry<K,V>[] oldTable = table;
				int oldCapacity = oldTable.length;
				int newCapacity = oldCapacity << 1;/* oldCapacity*2 */
				threshold = (int)(newCapacity * loadFactor);
				HashEntry<K,V>[] newTable =
					(HashEntry<K,V>[]) new HashEntry[newCapacity];
				int sizeMask = newCapacity - 1;
				for (int i = 0; i < oldCapacity ; i++) {
					HashEntry<K,V> e = oldTable[i];
					if (e != null) {
						HashEntry<K,V> next = e.next;
						int idx = e.hash & sizeMask;
						if (next == null)   //  Single node on list
							newTable[idx] = e;
						else { // Reuse consecutive sequence at same slot
							HashEntry<K,V> lastRun = e;
							int lastIdx = idx;
							for (HashEntry<K,V> last = next;
								 last != null;
								 last = last.next) {
								int k = last.hash & sizeMask;
								if (k != lastIdx) {
									lastIdx = k;
									lastRun = last;
								}
							}
							newTable[lastIdx] = lastRun;
							// Clone remaining nodes
							for (HashEntry<K,V> p = e; p != lastRun; p = p.next) {
								V v = p.value;
								int h = p.hash;
								int k = h & sizeMask;
								HashEntry<K,V> n = newTable[k];
								newTable[k] = new HashEntry<K,V>(h, p.key, v, n);
							}
						}
					}
				}
				int nodeIndex = node.hash & sizeMask; // add the new node
				node.setNext(newTable[nodeIndex]);
				newTable[nodeIndex] = node;
				table = newTable;
			}
			
		}
		
		
		//和put方法类似，基本上没什么区别；
		remove：{
			
			final V remove(Object key, int hash, Object value) {
				if (!tryLock())
					scanAndLock(key, hash);
				V oldValue = null;
				try {
					HashEntry<K,V>[] tab = table;
					int index = (tab.length - 1) & hash;
					HashEntry<K,V> e = entryAt(tab, index);
					HashEntry<K,V> pred = null;
					while (e != null) {
						K k;
						HashEntry<K,V> next = e.next;
						if ((k = e.key) == key ||
							(e.hash == hash && key.equals(k))) {
							V v = e.value;
							if (value == null || value == v || value.equals(v)) {
								if (pred == null)
									setEntryAt(tab, index, next);
								else
									pred.setNext(next);
								++modCount;
								--count;
								oldValue = v;
							}
							break;
						}
						pred = e;
						e = next;
					}
				} finally {
					unlock();
				}
				return oldValue;
			}
			
		}
		
		
		//因为size()对Segmen加锁，影响性能，用isEmpty()判断容量;
		//拿到所有的锁，才进行统计元素个数，
		size：{
			
		}
		
	}
		
	ConcurrentHashMap在1.8/中的实现：{
		
		ConcurrentHashMap和hashMap在实现上是链表加红黑树
		ConcurrentHashMap，1.8/CAS加Synchronized实现的，保证并发安全，也是弱一致性，
		
		treeNode  LinkedHashMap.entry 			hashMap
		treeNode  Node 							ConcurrenthashMap
			TreeNode<K,V> extends Node<K,V>
			红黑树的体现
		treeBin   Node							ConcurrenthashMap
			TreeBin<K,V> extends Node<K,V>
			
		扩容：
			ForwardingNode<K,V> extends Node<K,V>
			并发扩容：步长隔开，
		
		和新方法：定位设置节点
			1.7头插法，1.8尾插法；
		remove和put方法类似
		扩容比较麻烦，transfer：
		
		
		特殊的ForwardingNode：{
			一个特殊的 Node 结点，hash 值为 -1，其中存储 nextTable 的引用。有 table 发生扩容的时候，ForwardingNode 发挥作用，
				作为一个占位符放在 table 中表示当前结点为 null 或者已经被移动。
		}
		
		sizeCtl属性：{
			用来控制 table 的初始化和扩容操作。
			负数代表正在进行初始化或扩容操作
			-1代表正在初始化
			-N 表示有N-1个线程正在进行扩容操作
			0为默认值，代表当时的table还没有被初始化
			正数表示初始化大小或Map中的元素达到这个数量时，需要进行扩容了。

		}
		
		核心方法：{
			/*利用硬件级别的原子操作，获得在i位置上的Node节点
			* Unsafe.getObjectVolatile可以直接获取指定内存的数据，
			* 保证了每次拿到数据都是最新的*/
			static final <K,V> Node<K,V> tabAt(Node<K,V>[] tab, int i) {
				return (Node<K,V>)U.getObjectVolatile(tab, ((long)i << ASHIFT) + ABASE);
			}

			/*利用CAS操作设置i位置上的Node节点*/
			static final <K,V> boolean casTabAt(Node<K,V>[] tab, int i,
												Node<K,V> c, Node<K,V> v) {
				return U.compareAndSwapObject(tab, ((long)i << ASHIFT) + ABASE, c, v);
			}

			/*利用硬件级别的原子操作，设置在i位置上的Node节点
			* Unsafe.putObjectVolatile可以直接设定指定内存的数据，
			* 保证了其他线程访问这个节点时一定可以看到最新的数据*/
			static final <K,V> void setTabAt(Node<K,V>[] tab, int i, Node<K,V> v) {
				U.putObjectVolatile(tab, ((long)i << ASHIFT) + ABASE, v);
			}
		}
		
		构造方法：{
			public ConcurrentHashMap18(int initialCapacity,
                               float loadFactor, int concurrencyLevel) {
				if (!(loadFactor > 0.0f) || initialCapacity < 0 || concurrencyLevel <= 0)
					throw new IllegalArgumentException();
				if (initialCapacity < concurrencyLevel)   // Use at least as many bins
					initialCapacity = concurrencyLevel;   // as estimated threads
				long size = (long)(1.0 + (long)initialCapacity / loadFactor);
				int cap = (size >= (long)MAXIMUM_CAPACITY) ?
					MAXIMUM_CAPACITY : tableSizeFor((int)size);
				this.sizeCtl = cap;
			}
		}
		
		//get方法比较简单，给定一个key来确定value的时候，必须满足两个条件  key相同  hash值相同，对于节点可能在链表或树上的情况，需要分别去查找。
		get:{
			public V get(Object key) {
				Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
				int h = spread(key.hashCode());/*计算hash值*/
				/*根据hash值确定节点位置*/
				if ((tab = table) != null && (n = tab.length) > 0 &&
					(e = tabAt(tab, (n - 1) & h)) != null) {
					/*Node数组中的节点就是要找的节点*/
					if ((eh = e.hash) == h) {
						if ((ek = e.key) == key || (ek != null && key.equals(ek)))
							return e.val;
					}
					/*eh<0 说明这个节点在树上 调用树的find方法寻找*/
					else if (eh < 0)
						return (p = e.find(h, key)) != null ? p.val : null;
					/*到这一步说明是个链表，遍历链表找到对应的值并返回*/
					while ((e = e.next) != null) {
						if (e.hash == h &&
							((ek = e.key) == key || (ek != null && key.equals(ek))))
							return e.val;
					}
				}
				return null;
			}
		}
		
		//尾插法，从尾部开始插入值
		put：{
			//刚开始没有对table初始化，首次插入数值需要初始化数值，
			
			final V putVal(K key, V value, boolean onlyIfAbsent) {
				if (key == null || value == null) throw new NullPointerException();
				int hash = spread(key.hashCode());/*计算hash值*/
				int binCount = 0;
				/*死循环 何时插入成功 何时跳出*/
				for (Node<K,V>[] tab = table;;) {
					Node<K,V> f; int n, i, fh;
					if (tab == null || (n = tab.length) == 0)
						tab = initTable();/*如果table为空的话，初始化table*/
					else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
						/*Node数组中的元素，这个位置没有值 ，使用CAS操作放进去*/
						if (casTabAt(tab, i, null,
									 new Node<K,V>(hash, key, value, null)))
							break;                   // no lock when adding to empty bin
					}
					else if ((fh = f.hash) == MOVED)
						/*正在进行扩容，当前线程帮忙扩容*/
						tab = helpTransfer(tab, f);
					else {
						V oldVal = null;
						/*锁Node数组中的元素，这个位置是Hash冲突组成链表的头结点
						* 或者是红黑树的根节点*/
						synchronized (f) {
							if (tabAt(tab, i) == f) {
								/*fh>0 说明这个节点是一个链表的节点 不是树的节点*/
								if (fh >= 0) {
									binCount = 1;
									for (Node<K,V> e = f;; ++binCount) {
										K ek;
										/*put操作和putIfAbsent操作业务实现*/
										if (e.hash == hash &&
											((ek = e.key) == key ||
											 (ek != null && key.equals(ek)))) {
											oldVal = e.val;
											if (!onlyIfAbsent)
												e.val = value;
											break;
										}
										Node<K,V> pred = e;
										/*如果遍历到了最后一个结点，使用尾插法,把它插入在链表尾部*/
										if ((e = e.next) == null) {
											pred.next = new Node<K,V>(hash, key,
																	  value, null);
											break;
										}
									}
								}
								/*按照树的方式插入值*/
								else if (f instanceof TreeBin) {
									Node<K,V> p;
									binCount = 2;
									if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
																   value)) != null) {
										oldVal = p.val;
										if (!onlyIfAbsent)
											p.val = value;
									}
								}
							}
						}
						if (binCount != 0) {
							/*达到临界值8 就需要把链表转换为树结构*/
							if (binCount >= TREEIFY_THRESHOLD)
								treeifyBin(tab, i);
							if (oldVal != null)
								return oldVal;
							break;
						}
					}
				}
				/*Map的元素数量+1，并检查是否需要扩容*/
				addCount(1L, binCount);
				return null;
			}
		
			private final Node<K,V>[] initTable() {
				Node<K,V>[] tab; int sc;
				while ((tab = table) == null || tab.length == 0) {
					/*小于0表示有其他线程正在进行初始化操作，把当前线程CPU时间让出来。
					因为对于table的初始化工作，只能有一个线程在进行。*/
					if ((sc = sizeCtl) < 0)
						Thread.yield(); // lost initialization race; just spin
					/*利用CAS操作把sizectl的值置为-1 表示本线程正在进行初始化*/
					else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {
						try {
							if ((tab = table) == null || tab.length == 0) {
								int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
								@SuppressWarnings("unchecked")
								Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
								table = tab = nt;
								/*n右移2位本质上就是n变为n原值的1/4，所以
								* sc=0.75*n */
								sc = n - (n >>> 2);
							}
						} finally {
							/*将设置成扩容的阈值*/
							sizeCtl = sc;
						}
						break;
					}
				}
				return tab;
			}
			
		}
		
		//同put方法类似，
		remove：{
			移除方法的基本流程和put方法很类似，只不过操作由插入数据变为移除数据而已，而且如果存在红黑树的情况下，
				会检查是否需要将红黑树转为链表的步骤。不再重复讲述。

		}
		
		//扩容的时候如何实现并发扩容，
		transfer：{
			
		}
		
		size：{
			在JDK1.8版本中，对于size的计算，在扩容和addCount()方法就已经有处理了，可以注意一下Put函数，里面就有addCount()函数，
				早就计算好的，然后你size的时候直接给你。
			JDK1.7是在调用size()方法才去计算，其实在并发集合中去计算size是没有多大的意义的，因为size是实时在变的。

		}
		
		//一部分统计数值，另一部分统计是否需要扩容，
		addCount:{
			private final void addCount(long x, int check) {
				CounterCell[] as; long b, s;
				if ((as = counterCells) != null ||
					!U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) {
					CounterCell a; long v; int m;
					boolean uncontended = true;
					if (as == null || (m = as.length - 1) < 0 ||
						(a = as[ThreadLocalRandom.getProbe() & m]) == null ||
						!(uncontended =
						  U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) {
						fullAddCount(x, uncontended);
						return;
					}
					if (check <= 1)
						return;
					s = sumCount();
				}
				if (check >= 0) {
					Node<K,V>[] tab, nt; int n, sc;
					while (s >= (long)(sc = sizeCtl) && (tab = table) != null &&
						   (n = tab.length) < MAXIMUM_CAPACITY) {
						int rs = resizeStamp(n);
						if (sc < 0) {
							if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
								sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||
								transferIndex <= 0)
								break;
							if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))
								transfer(tab, nt);
						}
						else if (U.compareAndSwapInt(this, SIZECTL, sc,
													 (rs << RESIZE_STAMP_SHIFT) + 2))
							transfer(tab, null);
						s = sumCount();
					}
				}
			}
		}
		
		//在具体实现上，计算大小的核心方法都是 sumCount()
		可以看见，统计数量时使用了 baseCount、和CounterCell 类型的变量counterCells 。
			其实baseCount就是记录容器数量的，而counterCells则是记录CAS更新baseCounter值时，由于高并发而导致失败的值。
			这两个变量的变化在addCount() 方法中有体现，大致的流程就是：
			1/对 baseCount 做 CAS 自增操作。
			2/如果并发导致 baseCount CAS 失败了，则使用 counterCells。
			3/如果counterCells CAS 失败了，在 fullAddCount 方法中，会继续死循环操作，直到成功。

		sumCount:{
			final long sumCount() {
				CounterCell[] as = counterCells; CounterCell a;
				long sum = baseCount;
				if (as != null) {
					for (int i = 0; i < as.length; ++i) {
						if ((a = as[i]) != null)
							sum += a.value;
					}
				}
				return sum;
			}
		}
		
		
	}
	
	ConcurrentHashMap:都是弱一致性，
	
	并发下map的常见问题及面试题：{
		
		HashMap和HashTable的区别：{
			1/HashMap是线程不安全的；HashTable是线程安全的											//线程安全
			2/HashMap，效率快；HashTable，对整个map加锁，每个线程加上synchronized效率低下；			//效率问题
			3/HashMap，允许最多一个key为null和多个value为null；HashTable则不允许；					//keyvalue为null问题
			4/HashMap初始化容量为16，扩容缺省2倍；HashTable初始化容量为11，扩容2倍+1；				//扩容问题
			5/HashMap需要重新计算hash值；HashTable直接使用对象的hashCode。							//hash问题
		}

		java中的另一个线程安全的与HashMap极其类似的类是什么？同样是线程安全，它与HashTable在线程同步上有什么不同？{
			ConcurrentHashMap,是Java并发包java.util.concurrent中提供的一个线程安全且高效的HashMap实现;
			HashTable是使用 synchronize 关键字加锁的原理（就是对对象加锁）；
			而针对ConcurrentHashMap，在 JDK1.7 中采用分段锁的方式；JDK1.8 中直接采用了CAS（无锁算法）+synchronized，也采用分段锁的方式并大大缩小了锁的粒度。
		}
		
		//仔细补充一下
		HashMap & ConcurrentHashMap 的区别？{
			除了加锁，原理上无太大区别。
			另外，HashMap 的键值对允许有null，但是ConCurrentHashMap 都不允许。
			HashMap继承LinkedHashMap，ConcurrentHashMap继承Node,
			在数据结构上，红黑树相关的节点类

		}
	
		为什么ConcurrentHashMap比HashTable效率要高？{
			HashTable：
				使用一把锁（锁住整个链表结构，即锁的是整个map）处理并发问题，多个线程竞争一把锁，容易阻塞；线程竞争越激烈，效率越低，
			ConcurrentHashMap：  
				JDK 1.7 中使用分段锁（ReentrantLock + Segment + HashEntry），相当于把一个HashMap分成多个段，每段分配一把锁，这样支持多线程访问。
					锁粒度：基于Segment，包含多个HashEntry。
				JDK 1.8 中使用 CAS + synchronized + Node + 红黑树。锁粒度：Node（首结点）（实现 Map.Entry<K,V>）。锁粒度降低了。
		}
		
		针对 ConcurrentHashMap 锁机制具体分析（JDK 1.7 VS JDK 1.8）？{
			JDK1.7 中，采用分段锁的机制，实现并发的更新操作，底层采用数组+链表的存储结构，包括两个核心静态内部类Segment和HashEntry。
				①、Segment 继承 ReentrantLock（重入锁） 用来充当锁的角色，每个 Segment 对象守护每个散列映射表的若干个桶；
				②、HashEntry 用来封装映射表的键-值对；
				③、每个桶是由若干个 HashEntry 对象链接起来的链表。
			JDK 1.8 中，采用Node + CAS + Synchronized来保证并发安全。                            treeify threshold
				取消类 Segment，直接用 table 数组存储键值对；当 HashEntry 对象组成的链表长度超过 TREEIFY_THRESHOLD 时，链表转换为红黑树，提升性能。底层变更为数组 + 链表 + 红黑树。

		}
	
		ConcurrentHashMap 在 JDK 1.8 中，为什么要使用内置锁 synchronized 来代替重入锁 ReentrantLock？{
			1/JVM 开发团队在1.8中对 synchronized做了大量性能上的优化，而且基于 JVM 的 synchronized 优化空间更大，更加自然。
				显示锁是对象锁，消耗内存，
			2/在大量的数据操作下，对于 JVM 的内存压力，基于API的ReentrantLock会开销更多的内存。
		}
	
		ConcurrentHashMap 简单介绍？{
			①、重要的常量：
				private transient volatile int sizeCtl;
				当为负数时，
					-1 表示正在初始化，
					-N 表示 N - 1 个线程正在进行扩容；
				当为 0 时，表示 table 还没有初始化；
				当为其他正数时，表示初始化或者下一次进行扩容的大小。
			②、数据结构：
				Node 是存储结构的基本单元，继承 HashMap 中的 Entry，用于存储数据；
				TreeNode 继承 Node，但是数据结构换成了二叉树结构，是红黑树的存储结构，用于红黑树中存储数据；
				TreeBin 是封装 TreeNode 的容器，提供转换红黑树的一些条件和锁的控制。
			③、存储对象时（put() 方法）：
				1/如果没有初始化，就调用 initTable() 方法来进行初始化；
				2/如果没有 hash 冲突就直接 CAS 无锁插入；
				3/如果需要扩容，就先进行扩容；
				4/如果存在 hash 冲突，就加锁来保证线程安全，两种情况：一种是链表形式就直接遍历到尾端插入，一种是红黑树就按照红黑树结构插入；
				5/如果该链表的数量大于阀值 8，就要先转换成红黑树的结构，break 再一次进入循环
				6/如果添加成功就调用 addCount() 方法统计 size，并且检查是否需要扩容。
			④、扩容方法 transfer()：默认容量为 16，扩容时，容量变为原来的两倍。
				helpTransfer()：调用多个工作线程一起帮助进行扩容，这样的效率就会更高。
			⑤、获取对象时（get()方法）：
				1/计算 hash 值，定位到该 table 索引位置，如果是首结点符合就返回；
				2/如果遇到扩容时，会调用标记正在扩容结点 ForwardingNode.find()方法，查找该结点，匹配就返回；
				3/以上都不符合的话，就往下遍历结点，匹配就返回，否则最后就返回 null。

		}
	
		ConcurrentHashMap 的并发度是什么？{
			1.7/中程序运行时能够同时更新 ConcurentHashMap 且不产生锁竞争的最大线程数。
				默认为 16/且可以在构造函数中设置。当用户设置并发度时，ConcurrentHashMap 会使用大于等于该值的最小2幂指数作为实际并发度（假如用户设置并发度为17，实际并发度则为32）。
			1.8/中并发度则无太大的实际意义了，主要用处就是当设置的初始容量小于并发度，将初始容量提升至并发度大小。

		}
	
	}
	
	
	自己总结：{
		仔细了解掌握HashMap，并知道1.7/和1.8/HashMap的区别？
		仔细了解掌握ConcurrentHashMap，并知道1.7/和1.8/ConcurrentHashMap的区别？
		HashMap和ConcurrentHashMap？
	}
	

	
	
	
	
	
	更多的并发容器:
	
		ConcurrentSkipListMap  和 ConcurrentSkipListSet  存储有序的map、有序的set
			ConcurrentSkipListMap---->有序Map
			ConcurrentSkipListSet---->有序Set{
				
			TreeMap和TreeSet使用红黑树按照key的顺序（自然顺序、自定义顺序）来使得键值对有序存储，但是只能在单线程下安全使用，不是并发安全的；
				多线程下想要使键值对按照key的顺序来存储，则需要使用ConcurrentSkipListMap和ConcurrentSkipListSet，分别用以代替TreeMap和TreeSet，存入的数据按key排序。
				在实现上，ConcurrentSkipListSet 本质上就是ConcurrentSkipListMap，ConcurrentSkipListSet是对ConcurrentSkipListMap的包装。	
				
				
			SkipList:跳表，一种数据结构，概率数据结构，本质上是一钟空间换时间的算法，
				含义：传统意义的单链表是一个线性结构，向有序的链表中插入一个节点需要O(n)的时间，查找操作需要O(n)的时间。
				由于二分查找树--->链表----->平衡树、AVL、红黑树(B-,B+)	，于是出现跳表，
				先找一个大致的范围，再迫近，
				
				一个链表的查找速度：二分之n，
			
				哪些地方使用跳表了？
					redis和lueue开源引擎：都使用了跳表，
			
			为什么ConcurrentHashMap不使用跳表，
		}
		 
		
		ConcurrentLinkedQueue无界非阻塞队列，LinkedList的并发版本：{
			ConcurrentLinkedQueue内部基于链表的，是线程安全的。
			无界非阻塞队列，它是一个基于链表的无界线程安全队列。该队列的元素遵循先进先出的原则。头是最先加入的，尾是最近加入的。插入元素是追加到尾上。提取一个元素是从头提取。
			大家可以看成是LinkedList的并发版本，常用方法：
				concurrentLinkedQueue.add("c");    
				concurrentLinkedQueue.offer("d"); // 将指定元素插入到此队列的尾部。    
				concurrentLinkedQueue.peek(); // 检索并不移除此队列的头，如果此队列为空，则返回 null。    
				concurrentLinkedQueue.poll(); // 检索并移除此队列的头，如果此队列为空，则返回 null。

		}
		
		
		写时复制容器：{
			
			什么是写时复制容器：{
				CopyOnWrite容器即写时复制的容器。通俗的理解是当我们往一个容器添加元素的时候，不直接往当前容器添加，而是先将当前容器进行Copy，
					复制出一个新的容器，然后新的容器里添加元素，添加完元素之后，再将原容器的引用指向新的容器。
				这样做的好处是我们可以对CopyOnWrite容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。
					所以CopyOnWrite容器也是一种读写分离的思想，读和写不同的容器。
					如果读的时候有多个线程正在向CopyOnWriteArrayList添加数据，读还是会读到旧的数据，因为写的时候不会锁住旧的CopyOnWriteArrayList。
				CopyOnWrite并发容器用于对于绝大部分访问都是读，且只是偶尔写的并发场景。比如白名单，黑名单，商品类目的访问和更新场景，
					假如我们有一个搜索网站，用户在这个网站的搜索框中，输入关键字搜索内容，但是某些关键字不允许被搜索。
					这些不能被搜索的关键字会被放在一个黑名单当中，黑名单每天晚上更新一次。当用户搜索时，会检查当前关键字在不在黑名单当中，如果在，则提示不能搜索。
			}
			
			CopyOnWriteArrayList
			CopyOnWriteArraySet
				写时复制一个容器进行写入，读取时在旧的容器上进行读取数据，
				并发场景：读多写少的场景下，
					大多数读，偶尔写：白名单、黑名单；
				
			使用CopyOnWriteMap需要注意两件事情：
　　			1/减少扩容开销。根据实际需要，初始化CopyOnWriteMap的大小，避免写时CopyOnWriteMap扩容的开销。
				2/使用批量添加。因为每次添加，容器每次都会进行复制，所以减少添加次数，可以减少容器的复制次数。
			
			写时复制容器的问题：
				性能问题：
					每次修改都创建一个新数组，然后复制所有内容，如果数组比较大，修改操作又比较频繁，可以想象，性能是很低的，而且内存开销会很大。
				数据一致性问题
					CopyOnWrite容器只能保证数据的最终一致性，不能保证数据的实时一致性，是弱一致性。所以如果你希望写入的的数据，马上能读到，不要使用CopyOnWrite容器。

		}
		
		
		阻塞队列BlockingQueue：{
			容器肯定是有上限的，
			可以写三元运算符套三元运算符，
			
			队列：
				队列是一种特殊的线性表，特殊之处在于它只允许在表的前端（front）进行删除操作，而在表的后端（rear）进行插入操作，
					和栈一样，队列是一种操作受限制的线性表。进行插入操作的端称为队尾，进行删除操作的端称为队头。
				在队列中插入一个队列元素称为入队，从队列中删除一个队列元素称为出队。
					因为队列只允许在一端插入，在另一端删除，所以只有最早进入队列的元素才能最先从队列中删除，故队列又称为先进先出（FIFO—first in first out）线性表。
			
			什么是阻塞队列：{
				1）/支持阻塞的插入方法：意思是当队列满时，队列会阻塞插入元素的线程，直到队列不满。
				2）/支持阻塞的移除方法：意思是在队列为空时，获取元素的线程会等待队列变为非空。
				在并发编程中使用生产者和消费者模式能够解决绝大多数并发问题。该模式通过平衡生产线程和消费线程的工作能力来提高程序整体处理数据的速度。
				在线程世界里，生产者就是生产数据的线程，消费者就是消费数据的线程。在多线程开发中，如果生产者处理速度很快，而消费者处理速度很慢，
					那么生产者就必须等待消费者处理完，才能继续生产数据。同样的道理，如果消费者的处理能力大于生产者，那么消费者就必须等待生产者。
				为了解决这种生产消费能力不均衡的问题，便有了生产者和消费者模式。生产者和消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。
					生产者和消费者彼此之间不直接通信，而是通过阻塞队列来进行通信，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，
					消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。
				阻塞队列常用于生产者和消费者的场景，生产者是向队列里添加元素的线程，消费者是从队列里取元素的线程。阻塞队列就是生产者用来存放元素、消费者用来获取元素的容器。

				·抛出异常：当队列满时，如果再往队列里插入元素，会抛出IllegalStateException（"Queuefull"）异常。当队列空时，从队列里获取元素会抛出NoSuchElementException异常。
				·返回特殊值：当往队列插入元素时，会返回元素是否插入成功，成功返回true。如果是移除方法，则是从队列里取出一个元素，如果没有则返回null。
				·一直阻塞：当阻塞队列满时，如果生产者线程往队列里put元素，队列会一直阻塞生产者线程，直到队列可用或者响应中断退出。
					当队列空时，如果消费者线程从队列里take元素，队列会阻塞住消费者线程，直到队列不为空。
				·超时退出：当阻塞队列满时，如果生产者线程往队列里插入元素，队列会阻塞生产者线程一段时间，如果超过了指定的时间，生产者线程就会退出。

				
			}
					
			常用阻塞队列：{
				·ArrayBlockingQueue：一个由数组结构组成的有界阻塞队列。
				·LinkedBlockingQueue：一个由链表结构组成的有界阻塞队列。
				·PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。
				·DelayQueue：一个使用优先级队列实现的无界阻塞队列。
				·SynchronousQueue：一个不存储元素的阻塞队列。
				·LinkedTransferQueue：一个由链表结构组成的无界阻塞队列。
				·LinkedBlockingDeque：一个由链表结构组成的双向阻塞队列。
			}
	
			有界无界？
				有限队列就是长度有限，满了以后生产者会阻塞，无界队列就是里面能放无数的东西而不会因为队列长度限制被阻塞，当然空间限制来源于系统资源的限制，
					如果处理不及时，导致队列越来越大越来越大，超出一定的限制致使内存超限，操作系统或者JVM帮你解决烦恼，直接把你 OOM kill 省事了。

			ArrayBlockingQueue：{
				是一个用数组实现的有界阻塞队列。此队列按照先进先出（FIFO）的原则对元素进行排序。默认情况下不保证线程公平的访问队列，所谓公平访问队列是指阻塞的线程，
					可以按照阻塞的先后顺序访问队列，即先阻塞线程先访问队列。非公平性是对先等待的线程是非公平的，当队列可用时，阻塞的线程都可以争夺访问队列的资格，
					有可能先阻塞的线程最后才访问队列。初始化时有参数可以设置
			}
			
			LinkedBlockingQueue：{
				是一个用链表实现的有界阻塞队列。此队列的默认和最大长度为Integer.MAX_VALUE。此队列按照先进先出的原则对元素进行排序。
			}
			
			Array实现和Linked实现的区别：{
				1. 队列中锁的实现不同
					ArrayBlockingQueue实现的队列中的锁是没有分离的，即生产和消费用的是同一个锁；
					LinkedBlockingQueue实现的队列中的锁是分离的，即生产用的是putLock，消费是takeLock
				2. 在生产或消费时操作不同
					ArrayBlockingQueue实现的队列中在生产和消费的时候，是直接将枚举对象插入或移除的；
					LinkedBlockingQueue实现的队列中在生产和消费的时候，需要把枚举对象转换为Node<E>进行插入或移除，会影响性能
				3. 队列大小初始化方式不同
					ArrayBlockingQueue实现的队列中必须指定队列的大小；
					LinkedBlockingQueue实现的队列中可以不指定队列的大小，但是默认是Integer.MAX_VALUE
			}
			
			PriorityBlockingQueue：{
				PriorityBlockingQueue是一个支持优先级的无界阻塞队列。默认情况下元素采取自然顺序升序排列。
					也可以自定义类实现compareTo()方法来指定元素排序规则，或者初始化PriorityBlockingQueue时，指定构造参数Comparator来对元素进行排序。
					需要注意的是不能保证同优先级元素的顺序。
			}
			
			DelayQueue：{
				DelayQueue可以用消息中间件替换，不能用redis替换，缓存过期redis是没有提醒的。
				是一个支持延时获取元素的无界阻塞队列。队列使用PriorityQueue来实现。队列中的元素必须实现Delayed接口，在创建元素时可以指定多久才能从队列中获取当前元素。
					只有在延迟期满时才能从队列中提取元素。
				DelayQueue非常有用，可以将DelayQueue运用在以下应用场景。
					缓存系统的设计：可以用DelayQueue保存缓存元素的有效期，使用一个线程循环查询DelayQueue，一旦能从DelayQueue中获取元素时，表示缓存有效期到了。
					还有订单到期，限时支付等等
			}
			
			SynchronousQueue：{
				适用于传递性场景，
				是一个不存储元素的阻塞队列。每一个put操作必须等待一个take操作，否则不能继续添加元素。SynchronousQueue可以看成是一个传球手，
					负责把生产者线程处理的数据直接传递给消费者线程。队列本身并不存储任何元素，非常适合传递性场景。
					SynchronousQueue的吞吐量高于LinkedBlockingQueue和ArrayBlockingQueue。
			}
			
			
			LinkedTransferQueue：{
				多了tryTransfer和transfer方法，
				（1）transfer方法
					如果当前有消费者正在等待接收元素（消费者使用take()方法或带时间限制的poll()方法时），transfer方法可以把生产者传入的元素立刻transfer（传输）给消费者。
						如果没有消费者在等待接收元素，transfer方法会将元素存放在队列的tail节点，并等到该元素被消费者消费了才返回。
				（2）tryTransfer方法
					tryTransfer方法是用来试探生产者传入的元素是否能直接传给消费者。如果没有消费者等待接收元素，则返回false。
						和transfer方法的区别是tryTransfer方法无论消费者是否接收，方法立即返回，而transfer方法是必须等到消费者消费了才返回。
			}
			
			LinkedBlockingDeque：{
				LinkedBlockingDeque是一个由链表结构组成的双向阻塞队列。所谓双向队列指的是可以从队列的两端插入和移出元素。
					双向队列因为多了一个操作队列的入口，在多线程同时入队时，也就减少了一半的竞争。
				多了addFirst、addLast、offerFirst、offerLast、peekFirst和peekLast等方法，以First单词结尾的方法，表示插入、获取（peek）或移除双端队列的第一个元素。
					以Last单词结尾的方法，表示插入、获取或移除双端队列的最后一个元素。另外，插入方法add等同于addLast，移除方法remove等效于removeFirst。
					但是take方法却等同于takeFirst，不知道是不是JDK的bug，使用时还是用带有First和Last后缀的方法更清楚。
					在初始化LinkedBlockingDeque时可以设置容量防止其过度膨胀。另外，双向阻塞队列可以运用在“工作窃取”模式中。
			}
			
			了解阻塞队列的实现原理 
				使用了等待通知模式实现。所谓通知模式，就是当生产者往满的队列里添加元素时会阻塞住生产者，当消费者消费了一个队列中的元素后，会通知生产者当前队列可用。
					通过查看JDK源码发现ArrayBlockingQueue使用了Condition来实现。其余队列的实现，大家可以自行查看，队列的实现的代码总体来说，并不复杂。

						
			
		}
		

}



//线程池很重要，一定要研究透彻，
11/线程池：{
	
	总结：
		骨架：
			接口--->抽象类--->实现类；
		掌握快排，
		线程池是我们用得最多的并发框架，
		面试：必问线程池的工作机制、线程池参数、合理配置线程池：
	
	
	什么是线程池？为什么要使用线程池？
		含义：保存线程的池子
		1/使用线程池降低资源的消耗，降低线程创建和销毁的资源消耗；
		2/提高响应速度，线程的创建时间为T1，执行时间为T2，销毁时间T3，
		3/提高线程的可管理性：计算机中最宝贵最稀缺的资源就是线程，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。
		详细解释：{
			第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。
			第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。假设一个服务器完成一项任务所需时间为：T1 创建线程时间，T2 在线程中执行任务的时间，T3 销毁线程时间。   
				如果：T1 + T3 远大于 T2，则可以采用线程池，以提高服务器性能。线程池技术正是关注如何缩短或调整T1,T3时间的技术，从而提高服务器程序性能的。
				它把T1，T3分别安排在服务器程序的启动和结束的时间段或者一些空闲的时间段，这样在服务器程序处理客户请求时，不会有T1，T3的开销了。
			第三：提高线程的可管理性。
		}
		
		
		实现一个我们自己的线程池---手写一个线程池
			1/线程必须在池子已经创建好了，并且可以保持住，要有容器保存多个线程；
			2/线程还要能够接受外部的任务，运行这个任务。容器保持这个来不及运行的任务.
			缺点：线程无法调整数量，
		
		
	JDK中的线程池和工作机制
		线程池的创建
			ThreadPoolExecutor，jdk所有线程池实现的父类
			ThreadPoolExecutor
		interface	 interface
		Executor--->ExecutorService--->AbstractExecutorService--->ThreadPoolExecutor
		周期性的Executor--->ExecutorService--->ScheduledExecutorService--->ScheduledThreadPoolExecutor
		timer定时器线程不安全，
		quartz是spring的，
	ThreadPoolExecutor 的类关系:{
		Executor是一个接口，它是Executor框架的基础，它将任务的提交与任务的执行分离开来。
		ExecutorService接口继承了Executor，在其上做了一些shutdown()、submit()的扩展，可以说是真正的线程池接口；
		AbstractExecutorService抽象类实现了ExecutorService接口中的大部分方法；
		ThreadPoolExecutor是线程池的核心实现类，用来执行被提交的任务。
		ScheduledExecutorService接口继承了ExecutorService接口，提供了带"周期执行"功能ExecutorService；
		ScheduledThreadPoolExecutor是一个实现类，可以在给定的延迟后运行命令，或者定期执行命令。ScheduledThreadPoolExecutor比Timer更灵活，功能更强大。

	}
	
	
	线程池的创建各个参数：{
		
		corePoolSize：线程池中的核心线程数
			当提交一个任务时，线程池创建一个新线程执行任务，直到当前线程数等于corePoolSize；
			如果当前线程数为corePoolSize，继续提交的任务被保存到阻塞队列中，等待被执行；
			如果执行了线程池的prestartAllCoreThreads()方法，线程池会提前创建并启动所有核心线程。
		
		maximumPoolSize：线程池中允许的最大线程数。
			如果当前阻塞队列满了，且继续提交任务，则创建新的线程执行任务，前提是当前线程数小于maximumPoolSize
		
		keepAliveTime
			线程空闲时的存活时间，即当线程没有任务执行时，继续存活的时间。默认情况下，该参数只在线程数大于corePoolSize时才有用；
			只对corePoolSize之外，maximumPoolSize之内的线程起作用。
			TimeUnit
			keepAliveTime的时间单位
			workQueue
				workQueue必须是BlockingQueue阻塞队列。当线程池中的线程数超过它的corePoolSize的时候，线程会进入阻塞队列进行阻塞等待。
				通过workQueue，线程池实现了阻塞功能
			
		workQueue：用于保存等待执行的任务的阻塞队列
			一般来说，我们应该尽量使用有界队列，因为使用无界队列作为工作队列会对线程池带来如下影响。
				1/当线程池中的线程数达到corePoolSize后，新任务将在无界队列中等待，因此线程池中的线程数不会超过corePoolSize。
				2/由于1，使用无界队列时maximumPoolSize将是一个无效参数。
				3/由于1和2，使用无界队列时keepAliveTime将是一个无效参数。
				4/更重要的，使用无界queue可能会耗尽系统资源，有界队列则有助于防止资源耗尽；
					同时即使使用有界队列，也要尽量控制队列的大小在一个合适的范围。
			所以我们一般会使用，ArrayBlockingQueue、LinkedBlockingQueue、SynchronousQueue、PriorityBlockingQueue。
		
		threadFactory
			创建线程的工厂，通过自定义的线程工厂可以给每个新建的线程设置一个具有识别度的线程名，当然还可以更加自由的对线程做更多的设置，比如设置所有的线程为守护线程。
			参见代码cn.enjoyedu.ch6. ThreadPoolAdv
			Executors静态工厂里默认的threadFactory，线程的命名规则是“pool-数字-thread-数字”。
		
		RejectedExecutionHandler
			线程池的饱和策略，当阻塞队列满了，且没有空闲的工作线程，如果继续提交任务，必须采取一种策略处理该任务，
			  线程池提供了4种策略：---4/个缺省的拒绝策略，
				（1）AbortPolicy：直接抛出异常，默认策略；
				（2）CallerRunsPolicy：用调用者所在的线程来执行任务；谁调用谁执行
				（3）DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务；
				（4）DiscardPolicy：直接丢弃任务；
			当然也可以根据应用场景实现RejectedExecutionHandler接口，自定义饱和策略，
				如记录日志或持久化存储不能处理的任务，即将未处理的任务保存到数据库里等线程空闲的时候在执行该任务。
								
		设计目的思想：尽量少的线程执行尽量多的任务，	
		
	}

	
	提交任务：{
		execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功。
		submit()方法用于提交需要返回值的任务。线程池会返回一个future类型的对象，通过这个future对象可以判断任务是否执行成功，
			并且可以通过future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，
			而使用get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。

	}
	
	
	关闭线程池:{
		
	可以通过调用线程池的shutdown或shutdownNow方法来关闭线程池。它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止。
		但是它们存在一定的区别，shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表，
		而shutdown只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程。
	只要调用了这两个关闭方法中的任意一个，isShutdown方法就会返回true。当所有的任务都已关闭后，才表示线程池关闭成功，这时调用isTerminaed方法会返回true。
		至于应该调用哪一种方法来关闭线程池，应该由提交到线程池的任务特性决定，通常调用shutdown方法来关闭线程池，
		如果任务不一定要执行完，则可以调用shutdownNow方法。

	}
	
	
	合理配置线程池：
		根据任务的性质来：计算密集型（CPU），IO密集型，混合型
			计算密集型/CPU密集型：加密，大数分解，正则…….， 线程数适当小一点，最大推荐：机器的Cpu核心数+1，为什么+1，防止页缺失，
				(机器的Cpu核心=Runtime.getRuntime().availableProcessors();)
			IO密集型：读取文件，数据库连接，网络通讯, 线程数适当大一点，机器的Cpu核心数*2,
			混合型：尽量拆分，IO密集型>>计算密集型，拆分意义不大，IO密集型~计算密集型
			队列的选择上，应该使用有界，无界队列可能会导致内存溢出，OOM
		
	JDK提供的预定义线程池：
		FixedThreadPool
			创建固定线程数量的，适用于负载较重的服务器，使用了无界队列
		SingleThreadExecutor
			创建单个线程，需要顺序保证执行任务，不会有多个线程活动，使用了无界队列
		CachedThreadPool
			来一个任务就创建一个线程，不建议使用，底层是SynchronousQueue一个不存储元素的阻塞队列；
				当然了，如果任务数可以控制也是可以用的，阿里规范不建议使用的，但要具体情况具体分析。
			会根据需要来创建新线程的，执行很多短期异步任务的程序，使用了SynchronousQueue，
		WorkStealingPool（JDK7以后） 
			基于ForkJoinPool实现
		ScheduledThreadPoolExecutor 
			需要定期执行周期任务，Timer不建议使用了。
			newSingleThreadScheduledExecutor：只包含一个线程，只需要单个线程执行周期任务，保证顺序的执行各个任务
			newScheduledThreadPool 可以包含多个线程的，线程执行周期任务，适度控制后台线程数量的时候
			方法说明：
			schedule：只执行一次，任务还可以延时执行
			scheduleAtFixedRate：提交固定时间间隔的任务,
			scheduleWithFixedDelay：提交固定延时间隔执行的任务,不会超时的，
			两者的区别：

		
	
	Executor框架的执行流程：
		
	了解CompletionService：
		mark老师带着看过一遍了，
	
	tomcat的线程池：也是用FixedThreadPool实现的，并发量300-700；
	
	
	
}


	
12/线程安全：{
	
	类的线程安全定义  
		如果多线程下使用这个类，不过多线程如何使用和调度这个类，这个类总是表示出正确的行为，这个类就是线程安全的。
		类的线程安全表现为：
			操作的原子性
			内存的可见性
		不做正确的同步，在多个线程之间共享状态的时候，就会出现线程不安全。
 
		类的状态就是类的成员变量，
	
   
	怎么才能做到类的线程安全？{
		
		栈封闭
			所有的变量都是在方法内部声明的，这些变量都处于栈封闭状态。
			线程封闭：{
				实现好的并发是一件困难的事情，所以很多时候我们都想躲避并发。避免并发最简单的方法就是线程封闭。
				什么是线程封闭呢？
					就是把对象封装到一个线程里，只有这一个线程能看到此对象。那么这个对象就算不是线程安全的也不会出现任何安全问题。
				实现线程封闭有哪些方法呢？
					栈封闭：是我们编程当中遇到的最多的线程封闭。
						什么是栈封闭呢？简单的说就是局部变量。多个线程访问一个方法，此方法中的局部变量都会被拷贝一份到线程栈中。
							所以局部变量是不被多个线程所共享的，也就不会出现并发问题。所以能用局部变量就别用全局的变量，全局变量容易引起并发问题。
					ad-hoc线程封闭：
						这是完全靠实现者控制的线程封闭，他的线程封闭完全靠实现者实现。Ad-hoc线程封闭非常脆弱，应该尽量避免使用。

					TheadLocal
			}
			
		无状态的类：
			没有任何成员变量的类，就叫无状态的类，这种类一定是线程安全的。
			
		让类不可变：
			两种方式：
			1/加final关键字，对于一个类，所有的成员变量应该是私有的，同样的只要有可能，所有的成员变量应该加上final关键字，
				但是加上final，要注意如果成员变量又是一个对象时，这个对象所对应的类也要是不可变，才能保证整个类是不可变的。
			2/根本就不提供任何可供修改成员变量的地方，同时成员变量也不作为方法的返回值
			所有基本类型的包装类都是不可变的类，还有String，
			
		volatile：
			保证类的可见性，最适合一个线程写，多个线程读的情景；
			并不能保证类的线程安全性，只能保证类的可见性，最适合一个线程写，多个线程读的情景。
		加锁和CAS：
			我们最常使用的保证线程安全的手段，使用synchronized关键字，使用显式锁，使用各种原子变量，修改数据时使用CAS机制等等。
		安全的发布：
			类中持有的成员变量，特别是对象的引用，如果这个成员对象不是线程安全的，通过get等方法发布出去，会造成这个成员对象本身持有的数据在多线程下不正确的修改，
				从而造成整个类线程不安全的问题。
		TheadLocal：
			ThreadLocal是实现线程封闭的最好方法。ThreadLocal内部维护了一个Map，Map的key是每个线程的名称，而Map的值就是我们要封闭的对象。
				每个线程中的对象都对应着Map中一个值，也就是ThreadLocal利用Map实现了对象的线程封闭。
		Servlet：
			不是线程安全的类，Servlet在生命周期销毁；
			为什么我们平时没感觉到：
				1、在需求上，很少有共享的需求，
				2、接收到了请求，返回应答的时候，都是由一个线程来负责的。

		
		spring初始化IOC时，如何保证线程安全，加synchronized锁，没用ConcurrentHashMap；
			为什么使用synchronized，因为IOC初始化时，只做一次，后面就是读取bean了，
			从性能考虑没必要使用ConcurrentHashMap	
	}
	
	
	反射：spring，mabatis用的多，用反射的时候注意线程安全， 
	
	
	线程不安全引发的问题：{
		
		不是加锁了就能保证线程安全---例如死锁；
		死锁：是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，
				若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁。
			  必要条件：多个操作者争夺小于等于多个操作者的线程；
			  争夺资源锁的顺序不对。
			  
		解决：	
			线程不工作了，应用程序还在工作，没有错误(没有报错)，一旦发生异常，无法恢复，产生不可估量的错误。
			idea中照相机按钮可以查看死锁，
			死锁难在难复现，难定位；
			
		现象、危害和解决：
			现象：
				简单顺序死锁：参见代码cn.enjoyedu.ch7. NormalDeadLock
				动态顺序死锁
					顾名思义也是和获取锁的顺序有关，但是比较隐蔽，不像简单顺序死锁，往往从代码一眼就看出获取锁的顺序不对。
					参见代码cn.enjoyedu.ch7.tranfer.service. UserAccount
					
			危害:
				1、线程不工作了，但是整个程序还是活着的;
				2、没有任何的异常信息可以供我们检查;
				3、一旦程序发生了发生了死锁，是没有任何的办法恢复的，只能重启程序，对生产平台的程序来说，这是个很严重的问题。
			解决:
				定位
					要解决死锁，当然要先找到死锁，怎么找？
					通过jps 查询应用的 id，再通过jstack id 查看应用的锁的持有情况
				修正
					关键是保证拿锁的顺序一致
					两种解决方式
						内部通过顺序比较，确定拿锁的顺序；
						采用尝试拿锁的机制。
						参见代码cn.enjoyedu.ch7.tranfer.service. SafeOperate和SafeOperateToo
			
			实际工作中的死锁
			时间不定，不是每次必现；一旦出现没有任何异常信息，只知道这个应用的所有业务越来越慢，最后停止服务，无法确定是哪个具体业务导致的问题；测试部门也无法复现，并发量不够。
	}	
	
	
	其他安全问题：{
		活锁
			尝试拿锁的机制中，发生多个线程之间互相谦让，不断发生拿锁，释放锁的过程。
			解决办法：每个线程休眠随机数，错开拿锁的时间。
			
			两个线程在尝试拿锁的机制中，发生多个线程之间互相谦让，不断发生同一个线程总是拿到同一把锁，在尝试拿另一把锁时因为拿不到，
				而将本来已经持有的锁释放的过程。
			解决办法：每个线程休眠随机数，错开拿锁的时间。
			
		线程饥饿
			低优先级的线程，总是拿不到执行时间
			
	}
	
	
	性能和思考：{
		
		多线程会引发一些额外的开销，
		
		使用并发的目标是为了提高性能，引入多线程后，其实会引入额外的开销，如线程之间的协调、增加的上下文切换，线程的创建和销毁，线程的调度等等。
			过度的使用和不恰当的使用，会导致多线程程序甚至比单线程还要低。
		衡量应用的程序的性能：服务时间，延迟时间，吞吐量，可伸缩性等等，其中服务时间，延迟时间（多快），吞吐量（处理能力的指标，完成工作的多少）。
			多快和多少，完全独立，甚至是相互矛盾的。
		对服务器应用来说：多少（可伸缩性，吞吐量）这个方面比多快更受重视。
		我们做应用的时候：
		1、	先保证程序正确，确实达不到要求的时候，再提高速度。（黄金原则）
		2、	一定要以测试为基准。
		一个应用程序里，串行的部分是永远都有的。

		影响性能的因素：{
			上下文切换：线程池用尽量少的线程执行尽量多的任务，尽量少的线程就是为了减少上下文切换。
				是指CPU 从一个进程或线程切换到另一个进程或线程。一次上下文切换花费5000~10000个时钟周期，几微秒。
					在上下文切换过程中，CPU会停止处理当前运行的程序，并保存当前程序运行的具体位置以便之后继续运行。
					从这个角度来看，上下文切换有点像我们同时阅读几本书，在来回切换书本的同时我们需要记住每本书当前读到的页码。
				上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。
			内存同步:
				一般指加锁，对加锁来说，需要增加额外的指令，这些指令都需要刷新缓存等等操作。
			阻塞:
				会导致线程挂起【挂起：挂起进程在操作系统中可以定义为暂时被淘汰出内存的进程，机器的资源是有限的，在资源不足的情况下，操作系统对在内存中的程序进行合理的安排，
					其中有的进程被暂时调离出内存，当条件允许的时候，会被操作系统再次调回内存，重新进入等待被执行的状态即就绪态，系统在超过一定的时间没有任何动作】。
					很明显这个操作包括两次额外的上下文切换。
		}
		
	}
	
	
	减少锁的竞争：{
		减少锁的粒度
			使用锁的时候，锁所保护的对象是多个，当这些多个对象其实是独立变化的时候，不如用多个锁来一一保护这些对象。但是如果有同时要持有多个锁的业务方法，要注意避免发生死锁
		缩小锁的范围
			对锁的持有实现快进快出，尽量缩短持由锁的的时间。将一些与锁无关的代码移出锁的范围，特别是一些耗时，可能阻塞的操作
			避免多余的缩减锁的范围
			两次加锁之间的语句非常简单，导致加锁的时间比执行这些语句还长，这个时候应该进行锁粗化—扩大锁的范围。
		锁分段
			ConcurrrentHashMap就是典型的锁分段。
		替换独占锁 
			在业务允许的情况下：
			1、	使用读写锁，
			2、	用自旋CAS
			3、	使用系统的并发容器
	}
	
	
	线程安全的单例模式：
		双重检查锁定
		解决之道
			懒汉式
				类初始化模式，也叫延迟占位模式。在单例类的内部由一个私有静态内部类来持有这个单例类的实例。
				延迟占位模式还可以用在多线程下实例域的延迟赋值。
			饿汉式
				在声明的时候就new这个类的实例，因为在JVM中，对类的加载和类初始化，由虚拟机保证线程安全。
				或者使用枚举

				枚举也是饿汉式的一种方式，
				
			
}



架构师做架构设计和软件的开发，架构师就是在一个软件项目开发过程中，把客户的需求转换为规范的开发计划和文本，并且制定项目的总体架构，


实战项目-并发任务执行框架：{

	架构师是什么？{
		架构设计、软件开发 
			确认需求
				需求规格说明书必须得到架构师的认可。架构师需要和分析人员反复交流，以保证自己完整并准确地理解用户需求。
			系统分解
				依据用户需求，整个系统是否需要分层，如何进行分层,架构师将系统整体分解为更小的子系统和组件，从而形成不同的逻辑层或服务。
					随后，架构师会确定各层的接口，层与层相互之间的关系。架构师不仅要对整个系统分层，进行“纵向”分解，还要对同一逻辑层分块，进行“横向”分解。
					软件架构师的功力基本体现于此。
			技术选型
				架构师通过对系统的一系列的分解，最终形成了软件的整体架构。技术选择主要取决于软件架构，就是不断找到系统的瓶颈和弱点，
				采用分而治之、缓存、异步、集群等手段逐渐化解，并平衡处理系统各项要求（性能、安全、可用性、伸缩性、扩展性…）的过程。由此形成了架构。
			什么样的架构才是好的架构？
				答案：是适用于当前业务和团队成员，并保留适当前瞻性(最多半年的业务增长)的就是好架构。
			制定技术规格说明
				架构师在项目开发过程中，是技术权威。他需要协调所有的开发人员，与开发人员一直保持沟通，始终保证开发者依照它的架构意图去实现各项功能。

		开发管理 
			深深介入开发的方方面面
				规划产品路线，估算人力资源和时间资源，安排分工，确定里程碑点，指导工程师工作都需要架构师参与。
			
		沟通协调 
			与用户，与产品，与上级，与团队成员
				与用户沟通需求；与产品讨论需求，安排进度；与上级汇报进度，争取资源；团结团队成员等等

			
	}
	
	
	架构师的方方面面：{
		
		作用
			设计架构
				负责架构设计，负责架构的实施落地，演化发展，推广重构。
			救火
				系统出现难解决的故障，架构师出手解决。
			布道
				与他人分享自己的知识，培训团队成员。
		效果
			攻关
				挑战整个项目中最具技术难度和挑战性的模块。
			信念
				不管项目有多么艰难复杂，架构师在，团队成员就会坚信，项目一定能顺利完成。

		职责
			产品架构
				整个产品的技术架构
			基础服务架构
				负责开发基础框架，公共组件，通用服务等平台类产品。

	}
	

	我们需要做什么：{
		
		提高性能，采用多线程，屏蔽细节
			封装线程池和阻塞队列
		每个批量任务拥有自己的上下文环境
			需要一个并发安全的容器保存每个任务的具体信息
		自动清除已完成和过期任务
			定时轮询？不够优雅，使用延迟队列更好

		具体实现 – 可查询进度的并发任务执行框架

			用户业务方法的结果？

			如何执行用户的业务方法？
			  
			用户如何提交他的工作任务和查询任务进度？

	}
	
	
	
	
	实战：
		ITaskProcesser
		TaskResultType
		TaskResult 返回结果实体类，
		JobInfo：提交给框架执行的工作实体类，
	
	
	
	
	
	
	
	
	
}


//实现应用程序一个基本的黄金法则：先不考虑性能，先实现业务；实现完业务，性能确实不行，再进行优化；
//性能优化是建立在对业务的深度分析之上的，性能优化都用缓存、异步效果会更好，
实战项目-性能优化实战：{
	
	
	
}








JMM和底层实现原理：{
	
	JMM基础-计算机原理：{
	
		JMM(Java Memory Model):java内存模型，JMM定义了Java虚拟机(JVM)在计算机内存(RAM)中的工作方式。JVM是整个计算机虚拟模型，所以JMM是隶属于JVM的。
		早期计算机中cpu和内存的速度是差不多的，但在现代计算机中，cpu的指令速度远超内存的存取速度,由于计算机的存储设备与处理器的运算速度有几个数量级的差距，
			所以现代计算机系统都不得不加入一层读写速度尽可能接近处理器运算速度的高速缓存（Cache）来作为内存与处理器之间的缓冲：
			disrupt高速队列 百万级的吞吐量，BlockingQueue;
			将运算需要使用到的数据复制到缓存中，让运算能快速进行，当运算结束后再从缓存同步回内存之中，这样处理器就无须等待缓慢的内存读写了。
			1秒=1000毫秒，1毫秒=1000微秒，1微秒=1000纳秒，
		
		寄存器：{
			寄存器是中央处理器内的组成部分。寄存器是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和地址。在中央处理器的控制部件中，
				包含的寄存器有指令寄存器(IR)和程序计数器(PC)。在中央处理器的算术及逻辑部件中，寄存器有累加器(ACC)。
		}
		
		在计算机系统中，寄存器划是L0级缓存，接着依次是L1，L2，L3（接下来是内存，本地磁盘，远程存储）。
			越往上的缓存存储空间越小，速度越快，成本也更高；越往下的存储空间越大，速度更慢，成本也更低。
			从上至下，每一层都可以看做是更下一层的缓存，即：L0寄存器是L1一级缓存的缓存，L1是L2的缓存，依次类推；
			每一层的数据都是来至它的下一层，所以每一层的数据是下一层的数据的子集。
		
		在现代CPU上，一般来说L0， L1，L2，L3都集成在CPU内部，而L1还分为一级数据缓存（Data Cache，D-Cache，L1d）和一级指令缓存（Instruction Cache，I-Cache，L1i），
			分别用于存放数据和执行数据的指令解码。每个核心拥有独立的运算处理单元、控制器、寄存器、L1、L2缓存，然后一个CPU的多个核心共享最后一层CPU缓存L3
		
		工作内存：抽象概念，不是实际意义存在的。缓存，主内存的一部分，CPU中的寄存器，
	
	}
	
	内存屏障是java内存模型，
	volatile是最轻量的同步机制，
	
	
	重排序就是为了提高性能，
	
	物理内存模型带来的问题：{
		基于高速缓存的存储交互很好地解决了处理器与内存的速度矛盾，但是也为计算机系统带来更高的复杂度，因为它引入了一个新的问题：缓存一致性（Cache Coherence）。
			在多处理器系统中，每个处理器都有自己的高速缓存，而它们又共享同一主内存（MainMemory）。当多个处理器的运算任务都涉及同一块主内存区域时，
			将可能导致各自的缓存数据不一致。
		现代的处理器使用写缓冲区临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。
			同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，
			仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致。

	}
	
	伪共享：{
		前面我们已经知道，CPU中有好几级高速缓存。但是CPU缓存系统中是以缓存行（cache line）为单位存储的。目前主流的CPU Cache的Cache Line大小都是64Bytes。
			Cache Line可以简单的理解为CPU Cache中的最小缓存单位，今天的CPU不再是按字节访问内存，而是以64字节为单位的块(chunk)拿取，称为一个缓存行(cache line)。
			当你读一个特定的内存地址，整个缓存行将从主存换入缓存。
		一个缓存行可以存储多个变量（存满当前缓存行的字节数）；而CPU对缓存的修改又是以缓存行为最小单位的，在多线程情况下，如果需要修改“共享同一个缓存行的变量”，
			就会无意中影响彼此的性能，这就是伪共享（False Sharing）。
		为了避免伪共享，我们可以使用数据填充的方式来避免，即单个数据填充满一个CacheLine。这本质是一种空间换时间的做法。但是这种方式在Java7以后可能失效。
		
		
		
	}
	
	Java内存模型（JMM）：{
		从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（Main Memory）中，每个线程都有一个私有的本地内存（Local Memory），
			本地内存中存储了该线程以读/写共享变量的副本。本地内存是JMM的一个抽象概念，并不真实存在。它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译器优化。
	}
	
	Java内存模型带来的问题：{
		
		可见性问题：
		
		竞争问题：
		
		重排序：{
			重排序类型
				除了共享内存和工作内存带来的问题，还存在重排序的问题：在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。重排序分3种类型。
				1/编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。
				2/指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-LevelParallelism，ILP）来将多条指令重叠执行。
					如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。
				3/内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。
			数据依赖性
				数据依赖性：如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。
					数据依赖分为下列3种类型，上面3种情况，只要重排序两个操作的执行顺序，程序的执行结果就会被改变。
			as-if-serial
				as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。
					编译器、runtime和处理器都必须遵守as-if-serial语义。
				为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。
					（强调一下，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。）
					但是，如果操作之间不存在数据依赖关系，这些操作依然可能被编译器和处理器重排序。
				A和C之间存在数据依赖关系，同时B和C之间也存在数据依赖关系。因此在最终执行的指令序列中，C不能被重排序到A和B的前面（C排到A和B的前面，程序的结果将会被改变）。
					但A和B之间没有数据依赖关系，编译器和处理器可以重排序A和B之间的执行顺序。
				as-if-serial语义把单线程程序保护了起来，遵守as-if-serial语义的编译器、runtime和处理器可以让我们感觉到：单线程程序看起来是按程序的顺序来执行的。
					asif-serial语义使单线程程序员无需担心重排序会干扰他们，也无需担心内存可见性问题。	
			
			控制依赖性：
				
			
		}
		
		
		内存屏障：
			Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序，从而让程序按我们预想的流程去执行。
				1/保证特定操作的执行顺序。
				2/影响某些数据（或则是某条指令的执行结果）的内存可见性。
			编译器和CPU能够重排序指令，保证最终相同的结果，尝试优化性能。插入一条Memory Barrier会告诉编译器和CPU：不管什么指令都不能和这条Memory Barrier指令重排序。
			Memory Barrier所做的另外一件事是强制刷出各种CPU cache，如一个Write-Barrier（写入屏障）将刷出所有在Barrier之前写入 cache 的数据，
				因此，任何CPU上的线程都能读取到这些数据的最新版本。
			JMM把内存屏障指令分为4类
				StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多处理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。
			
		临界区：
			JMM会在退出临界区和进入临界区这两个关键时间点做一些特别处理，使得多线程在这两个时间点按某种顺序执行。
			临界区内的代码则可以重排序（但JMM不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。
				虽然线程A在临界区内做了重排序，但由于监视器互斥执行的特性，这里的线程B根本无法“观察”到线程A在临界区内的重排序。
				这种重排序既提高了执行效率，又没有改变程序的执行结果。
			回想一下，为啥线程安全的单例模式中一般的双重检查不能保证真正的线程安全？

		
	}
	
	happens-before：{
		在Java 规范提案中为让大家理解内存可见性的这个概念，提出了happens-before的概念来阐述操作之间的内存可见性。
			对应Java程序员来说，理解happens-before是理解JMM的关键。
		JMM这么做的原因是：程序员对于这两个操作是否真的被重排序并不关心，程序员关心的是程序执行时的语义不能被改变（即执行结果不能被改变）。
			因此，happens-before关系本质上和as-if-serial语义是一回事。as-if-serial语义保证单线程内程序的执行结果不被改变，
			happens-before关系保证正确同步的多线程程序的执行结果不被改变。
		定义：
			用happens-before的概念来阐述操作之间的内存可见性。在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在happens-before关系 。 
			两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，
				且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）
		
		加深理解：
			上面的定义看起来很矛盾，其实它是站在不同的角度来说的。
			1/站在Java程序员的角度来说：JMM保证，如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，
				而且第一个操作的执行顺序排在第二个操作之前。
			2/站在编译器和处理器的角度来说：JMM允许，两个操作之间存在happens-before关系，不要求Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。
				如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这种重排序是允许的。
			
		
		Happens-Before规则
			JMM为我们提供了以下的Happens-Before规则：
			1/程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。
			2/监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。
			3/volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。
			4/传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。
			5/start()规则：如果线程A执行操作ThreadB.start()（启动线程B），那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。
			6/join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 
			7/线程中断规则:对线程interrupt方法的调用happens-before于被中断线程的代码检测到中断事件的发生。
					
				
	}
	
	volatile详解：{
		
		volatile特性：
			可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。
			原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性。
			
		volatile内存语义：
			内存语义：可以简单理解为 volatile，synchronize，atomic，lock 之类的在 JVM 中的内存方面实现原则。
			volatile写的内存语义如下：
				当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存。 
			volatile读的内存语义如下：
				当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 
			
				
		为何volatile不是线程安全的：
			保证可见性，不能保证原子性。
		
		volatile内存语义的实现：{
			
			volatile重排序规则表：
				总结起来就是：
					当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。
					当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。
					当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。
			volatile的内存屏障：
				在Java中对于volatile修饰的变量，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序问题。
				volatile写：
					storestore屏障：对于这样的语句store1; storestore; store2，在store2及后续写入操作执行前，保证store1的写入操作对其它处理器可见。
						(也就是说如果出现storestore屏障，那么store1指令一定会在store2之前执行，CPU不会store1与store2进行重排序)
					storeload屏障：对于这样的语句store1; storeload; load2，在load2及后续所有读取操作执行前，保证store1的写入对所有处理器可见。
						(也就是说如果出现storeload屏障，那么store1指令一定会在load2之前执行,CPU不会对store1与load2进行重排序)
				volatile读：
					在每个volatile读操作的后面插入一个LoadLoad屏障。在每个volatile读操作的后面插入一个loadstore屏障。
					•  loadload屏障：对于这样的语句load1; loadload; load2，在load2及后续读取操作要读取的数据被访问前，保证load1要读取的数据被读取完毕。
						（也就是说，如果出现loadload屏障，那么load1指令一定会在load2之前执行，CPU不会对load1与load2进行重排序） 
					•  loadstore屏障：对于这样的语句load1; loadstore; store2，在store2及后续写入操作被刷出前，保证load1要读取的数据被读取完毕。
						（也就是说，如果出现loadstore屏障，那么load1指令一定会在store2之前执行，CPU不会对load1与store2进行重排序）

			
		}
		
		volatile的实现原理：{
			通过对OpenJDK中的unsafe.cpp源码的分析，会发现被volatile关键字修饰的变量会存在一个“lock:”的前缀。
			Lock前缀，Lock不是一种内存屏障，但是它能完成类似内存屏障的功能。Lock会对CPU总线和高速缓存加锁，可以理解为CPU指令级的一种锁。
			同时该指令会将当前处理器缓存行的数据直接写会到系统内存中，且这个写回内存的操作会使在其他CPU里缓存了该地址的数据无效。
			在具体的执行上，它先对总线和缓存加锁，然后执行后面的指令，最后释放锁后会把高速缓存中的脏数据全部刷新回主内存。
				在Lock锁住总线的时候，其他CPU的读写请求都会被阻塞，直到锁释放。

		}
			
			
		
	}
	
	
	
	final的内存语义：{
		在构造线程的类时，我们有种方式就是让类中所有的成员变量都不可变，利用的就是final关键字，那么这个final为何可以做到呢？
			重排序这种优化动作对构造方法，一样也是存在的。这就说明，一个成员变量加了final关键字后，JMM一定是做了相关处理的。
		
		final的两个重排序规则：{
			
		}
		
		final域为引用类型：{
			
		}
		
		final引用不能从构造函数内逃逸：{
			
		}
		
		final语义的实现：{
			会要求编译器在final域的写之后，构造函数return之前插入一个StoreStore障屏。
			读final域的重排序规则要求编译器在读final域的操作前面插入一个LoadLoad屏障

		}
		
		
		
	}
	
	
	锁的内存语义：{
		当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中。
		当线程获取锁时，JMM会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须从主内存中读取共享变量。 

		
		
		
		
	}
	
	
	synchronized的实现原理：{
		Synchronized在JVM里的实现都是基于进入和退出Monitor对象来实现方法同步和代码块同步，虽然具体实现细节不一样，但是都可以通过成对的MonitorEnter和MonitorExit指令来实现。
		对同步块，MonitorEnter指令插入在同步代码块的开始位置，当代码执行到该指令时，将会尝试获取该对象Monitor的所有权，即尝试获得该对象的锁，
			而monitorExit指令则插入在方法结束处和异常处，JVM保证每个MonitorEnter必须有对应的MonitorExit。
		对同步方法，从同步方法反编译的结果来看，方法的同步并没有通过指令monitorenter和monitorexit来实现，相对于普通方法，其常量池中多了ACC_SYNCHRONIZED标示符。
		JVM就是根据该标示符来实现方法的同步的：当方法被调用时，调用指令将会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先获取monitor，
			获取成功之后才能执行方法体，方法执行完后再释放monitor。在方法执行期间，其他任何线程都无法再获得同一个monitor对象。
		synchronized使用的锁是存放在Java对象头里面，
		
	}
	
	了解各种锁：{
		
		自旋锁：{
			原理
				自旋锁原理非常简单，如果持有锁的线程能在很短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞挂起状态，
					它们只需要等一等（自旋），等持有锁的线程释放锁后即可立即获取锁，这样就避免用户线程和内核的切换的消耗。
				但是线程自旋是需要消耗CPU的，说白了就是让CPU在做无用功，线程不能一直占用CPU自旋做无用功，所以需要设定一个自旋等待的最大时间。
				如果持有锁的线程执行的时间超过自旋等待的最大时间扔没有释放锁，就会导致其它争用锁的线程在最大等待时间内还是获取不到锁，这时争用线程会停止自旋进入阻塞状态。

			自旋锁的优缺点
				自旋锁尽可能的减少线程的阻塞，这对于锁的竞争不激烈，且占用锁时间非常短的代码块来说性能能大幅度的提升，因为自旋的消耗会小于线程阻塞挂起操作的消耗！
				但是如果锁的竞争激烈，或者持有锁的线程需要长时间占用锁执行同步块，这时候就不适合使用自旋锁了，因为自旋锁在获取锁前一直都是占用cpu做无用功，占着XX不XX，
					线程自旋的消耗大于线程阻塞挂起操作的消耗，其它需要cup的线程又不能获取到cpu，造成cpu的浪费。
	
			自旋锁时间阈值
				自旋锁的目的是为了占着CPU的资源不释放，等到获取到锁立即进行处理。但是如何去选择自旋的执行时间呢？如果自旋执行时间太长，会有大量的线程处于自旋状态占用CPU资源，
					进而会影响整体系统的性能。因此自旋次数很重要
				JVM对于自旋次数的选择，jdk1.5/默认为10次，在1.6/引入了适应性自旋锁，适应性自旋锁意味着自旋的时间不在是固定的了，
					而是由前一次在同一个锁上的自旋时间以及锁的拥有者的状态来决定，基本认为一个线程上下文切换的时间是最佳的一个时间。
				JDK1.6中-XX:+UseSpinning开启自旋锁； JDK1.7后，去掉此参数，由jvm控制；	
				
		}
		
		锁的状态：{
			一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。锁可以升级但不能降级，目的是为了提高获得锁和释放锁的效率。
		}
		
		偏向锁：{
			
		}
		
		轻量级锁：{
			
		}
		
		不同锁的比较：{
			
		}
		
		JDK对锁的更多优化措施：{
			逃逸分析
				如果证明一个对象不会逃逸方法外或者线程外，则可针对此变量进行优化：
				同步消除synchronization Elimination，如果一个对象不会逃逸出线程，则对此变量的同步措施可消除。
			锁消除和粗化
				锁消除：虚拟机的运行时编译器在运行时如果检测到一些要求同步的代码上不可能发生共享数据竞争，则会去掉这些锁。
				锁粗化：将临近的代码块用同一个锁合并起来。
				消除无意义的锁获取和释放，可以提高程序运行性能。

		}
		
		
	}
	
	
	
}


Java8新增特性：{
	
	新增原子操作：{
		LongAdder：更快的原子类
			设计思想：分离热点

	}

	新增显式锁：{
		
		StampLock：读写锁的改进
			ReentrantReadWriteLock 在沒有任何读写锁时，才可以取得写入锁，读取执行情况很多，写入很少的情况下，
				使用 ReentrantReadWriteLock 可能会使写入线程遭遇饥饿（Starvation）问题
			StampedLock则提供了一种乐观的读策略,这种乐观策略的锁非常类似于无锁的操作,使得乐观锁完全不会阻塞写线程
		
		
			
	}

	CompleteableFuture：{
		Future的不足：
			结果的获取不方便
			很难直接表述多个Future 结果之间的依赖性
		CompleteableFuture：
			实现了Future<T>, CompletionStage<T>两个接口
			基本用法

		多线程并发，取结果归集的几种实现方案：
			原理		Future接口			



		
	}

	
	扩充知识点-Disruptor：{
		英国外汇交易公司LMAX开发的一个高性能队列，是一个高性能的线程间异步通信的框架，即在同一个JVM进程中的多线程间消息传递，它不是分布式队列。
			基于Disruptor开发的系统单线程能支撑每秒600万订单。
		应用Disruptor的知名项目有如下的一些：Storm, Camel, Log4j2,还有目前的美团点评技术团队
		
		
		传统队列的问题
		高性能的原理

	}

	
	
}



并发编程复习总结与面试：{
	
	在java中守护线程和用户线程的区别？{
		java中的线程分为两种：守护线程（Daemon）和用户线程（User）。
		任何线程都可以设置为守护线程和用户线程，通过方法Thread.setDaemon(bool on)；true则把该线程设置为守护线程，反之则为用户线程。
			Thread.setDaemon()必须在Thread.start()之前调用，否则运行时会抛出异常。
		两者的区别： 
		唯一的区别是判断虚拟机(JVM)何时离开，Daemon是为其他线程提供服务，如果全部的User Thread已经结束，Daemon 没有可服务的线程，JVM关闭。
		扩展：Thread Dump打印出来的线程信息，含有daemon字样的线程即为守护进程

	}
	
	线程与进程的区别：{
		进程是操作系统分配资源的最小单元，线程是操作系统调度的最小单元。
		一个程序至少有一个进程,一个进程至少有一个线程。

	}
	
	什么是多线程中的上下文切换：{
		多线程会共同使用一组计算机上的CPU，而线程数大于给程序分配的CPU数量时，为了让各个线程都有执行的机会，就需要轮转使用CPU。
			不同的线程切换使用CPU发生的切换数据等就是上下文切换。
	}
	
	死锁与活锁的区别，死锁与饥饿的区别？：{
		死锁：是指两个或两个以上的进程（或线程）在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。 
		产生死锁的必要条件： 
		互斥条件：所谓互斥就是进程在某一时间内独占资源。
		请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 
		不剥夺条件:进程已获得资源，在末使用完之前，不能强行剥夺。 
		循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。
		活锁：任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。
		活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，所谓的“活”， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。
		饥饿：一个或者多个线程因为种种原因无法获得所需要的资源，导致一直无法执行的状态。

	}
	
	synchronized底层实现原理：{
		synchronized (this)原理：涉及两条指令：monitorenter，monitorexit；再说同步方法，从同步方法反编译的结果来看，
			方法的同步并没有通过指令monitorenter和monitorexit来实现，相对于普通方法，其常量池中多了ACC_SYNCHRONIZED标示符。

		JVM就是根据该标示符来实现方法的同步的：当方法被调用时，调用指令将会检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，
			执行线程将先获取monitor，获取成功之后才能执行方法体，方法执行完后再释放monitor。在方法执行期间，其他任何线程都无法再获得同一个monitor对象。
		注意，这个问题可能会接着追问，java对象头信息，偏向锁，轻量锁，重量级锁及其他们相互间转化。

	}
	
	什么是线程组，为什么在Java中不推荐使用？：{
		ThreadGroup类，可以把线程归属到某一个线程组中，线程组中可以有线程对象，也可以有线程组，组中还可以有线程，这样的组织结构有点类似于树的形式。
		1/线程组ThreadGroup对象中比较有用的方法是stop、resume、suspend等方法，由于这几个方法会导致线程的安全问题（主要是死锁问题），
			已经被官方废弃掉了，所以线程组本身的应用价值就大打折扣了。
		2/线程组ThreadGroup不是线程安全的，这在使用过程中获取的信息并不全是及时有效的，这就降低了它的统计使用价值。

	}
	
	什么是Executors框架？为什么使用Executor框架？：{
		Executor框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。
		每次执行任务创建线程 new Thread()比较消耗性能，创建一个线程是比较耗时、耗资源的。
		调用 new Thread()创建的线程缺乏管理，而且可以无限制的创建，线程之间的相互竞争会导致过多占用系统资源而导致系统瘫痪，
			还有线程之间的频繁交替也会消耗很多系统资源。
		接使用new Thread() 启动的线程不利于扩展，比如定时执行、定期执行、定时定期执行、线程中断等都不便实现。

	}
	
	在Java中Executor和Executors的区别？：{
		Executors 工具类的不同方法按照我们的需求创建了不同的线程池，来满足业务的需求。 
		Executor 接口对象能执行我们的线程任务。 
		ExecutorService接口继承了Executor接口并进行了扩展，提供了更多的方法我们能获得任务执行的状态并且可以获取任务的返回值。 
		使用ThreadPoolExecutor 可以创建自定义线程池。

	}
	
	什么是原子操作？在Java Concurrency API中有哪些原子类(atomic classes)？：{
		原子操作（atomic operation）意为”不可被中断的一个或一系列操作” 。 
		处理器使用基于对缓存加锁或总线加锁的方式来实现多处理器之间的原子操作。 

		在Java中可以通过锁和循环CAS的方式来实现原子操作。 CAS操作——Compare & Set，或是 Compare & Swap，现在几乎所有的CPU指令都支持CAS的原子操作。
			java.util.concurrent.atomic下提供了大量的原子操作类，比如原子类：AtomicBoolean，AtomicInteger，AtomicLong，AtomicReference ，
			原子数组：AtomicIntegerArray，AtomicLongArray，AtomicReferenceArray ，
			原子属性更新器：AtomicLongFieldUpdater，AtomicIntegerFieldUpdater，AtomicReferenceFieldUpdater

	}
	
	Java Concurrency API中的Lock接口(Lock interface)是什么？对比synchronized它有什么优势？：{
		Lock接口比同步方法和同步块提供了更具扩展性的锁操作。 
		他们允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。
		它的优势有：可以使锁更公平，可以使线程在等待锁的时候响应中断，可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间，
			可以在不同的范围，以不同的顺序获取和释放锁。
		整体上来说Lock是synchronized的扩展版，Lock提供了无条件的、可轮询的(tryLock方法)、定时的(tryLock带参方法)、可中断的(lockInterruptibly)、
			可多条件队列的(newCondition方法)锁操作。另外Lock的实现类基本都支持非公平锁(默认)和公平锁，synchronized只支持非公平锁，当然，在大部分情况下，非公平锁是高效的选择。

	}
	
	
	什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？：{
		阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。
		这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。
		阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。
			阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。
		JDK7提供了7个阻塞队列。在实现上，主要是利用了Condition和Lock的等待通知模式。

	}
	
	什么是Callable和Future?：{
		Callable接口类似于Runnable，从名字就可以看出来了，但是Runnable不会返回结果，并且无法抛出返回结果的异常，
			而Callable功能更强大一些，被线程执行后，可以返回值，这个返回值可以被Future拿到，也就是说，Future可以拿到异步执行任务的返回值。 
		可以认为是带有回调的Runnable。
		Future接口表示异步任务，是还没有完成的任务给出的未来结果。所以说Callable用于产生结果，Future用于获取结果。

	}
	
	什么是FutureTask?：{
		在Java并发程序中FutureTask表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。
			只有当运算完成的时候结果才能取回，如果运算尚未完成get方法将会阻塞。
			一个FutureTask对象可以对调用了Callable和Runnable的对象进行包装，由于FutureTask也是调用了Runnable接口所以它可以提交给Executor来执行。
	}
	
	什么是并发容器的实现？：{
		何为同步容器：可以简单地理解为通过synchronized来实现同步的容器，如果有多个线程调用同步容器的方法，它们将会串行执行。
			比如Vector，Hashtable，以及Collections.synchronizedSet，synchronizedList等方法返回的容器。 
		并发容器使用了与同步容器完全不同的加锁策略来提供更高的并发性和伸缩性，例如在ConcurrentHashMap中采用了一种粒度更细的加锁机制，可以称为分段锁，
			在这种锁机制下，允许任意数量的读线程并发地访问map，并且执行读操作的线程和写操作的线程也可以并发的访问map，
			同时允许一定数量的写操作线程并发地修改map，所以它可以在并发环境下实现更高的吞吐量。

	}
	
	多线程同步和互斥有几种实现方法，都是什么？：{
		线程同步是指线程之间所具有的一种制约关系，一个线程的执行依赖另一个线程的消息，当它没有得到另一个线程的消息时应等待，直到消息到达时才被唤醒。 
		线程互斥是指对于共享的进程系统资源，在各单个线程访问时的排它性。当有若干个线程都要使用某一共享资源时，任何时刻最多只允许一个线程去使用，
			其它要使用该资源的线程必须等待，直到占用资源者释放该资源。线程互斥可以看成是一种特殊的线程同步。
		线程间的同步方法大体可分为两类：用户模式和内核模式。顾名思义，内核模式就是指利用系统内核对象的单一性来进行同步，使用时需要切换内核态与用户态，
			而用户模式就是不需要切换到内核态，只在用户态完成操作。 
		用户模式下的方法有：原子操作（例如一个单一的全局变量），临界区。内核模式下的方法有：事件，信号量，互斥量。

	}
	
	什么是竞争条件？：{
		当多个进程都企图对共享数据进行某种处理，而最后的结果又取决于进程运行的顺序时，则我们认为这发生了竞争条件（race condition）。
	}
	
	为什么我们调用start()方法时会执行run()方法，为什么我们不能直接调用run()方法？：{
		当你调用start()方法时你将创建新的线程，并且执行在run()方法里的代码。 
		但是如果你直接调用run()方法，它不会创建新的线程也不会执行调用线程的代码，只会把run方法当作普通方法去执行。

	}
	
	在Java中CycliBarriar和CountdownLatch有什么区别？：{
		CyclicBarrier可以重复使用，而CountdownLatch不能重复使用。
	}
	
	什么是不可变对象，它对写并发应用有什么帮助？：{
		不可变对象(Immutable Objects)即对象一旦被创建它的状态（对象的数据，也即对象属性值）就不能改变，反之即为可变对象(Mutable Objects)。 
		不可变对象的类即为不可变类(Immutable Class)。Java平台类库中包含许多不可变类，如String、基本类型的包装类、BigInteger和BigDecimal等。 
		不可变对象天生是线程安全的。它们的常量（域）是在构造函数中创建的。既然它们的状态无法修改，这些常量永远不会变。
		不可变对象永远是线程安全的。 
		只有满足如下状态，一个对象才是不可变的； 
		它的状态不能在创建后再被修改； 
		所有域都是final类型；并且， 
		它被正确创建

	}
	
	notify()和notifyAll()有什么区别？：{
		当一个线程进入wait之后，就必须等其他线程notify/notifyall,使用notifyall,可以唤醒所有处于wait状态的线程，使其重新进入锁的争夺队列中，
			而notify只能唤醒一个。
		如果没把握，建议notifyAll，防止notigy因为信号丢失而造成程序异常。

	}
	
	
	什么是可重入锁（ReentrantLock）？谈谈它的实现。：{
		线程可以重复进入任何一个它已经拥有的锁所同步着的代码块，synchronized、ReentrantLock都是可重入的锁。
			在实现上，就是线程每次获取锁时判定如果获得锁的线程是它自己时，简单将计数器累积即可，每 释放一次锁，进行计数器累减，
			直到计算器归零，表示线程已经彻底释放锁。
	}
	
	当一个线程进入某个对象的一个synchronized的实例方法后，其它线程是否可进入此对象的其它方法？：{
		如果其他方法没有synchronized的话，其他线程是可以进入的。
		所以要开放一个线程安全的对象时，得保证每个方法都是线程安全的。

	}
	
	乐观锁和悲观锁的理解及如何实现，有哪些实现方式？：{
		悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。
			Java里面的同步原语synchronized关键字的实现是悲观锁。
		乐观锁：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，
			可以使用版本号等机制。在Java中j原子变量类就是使用了乐观锁的一种实现方式CAS实现的。
		乐观锁的实现方式： 
		•	使用版本标识来确定读到的数据与提交时的数据是否一致。提交后修改版本标识，不一致时可以采取丢弃和再次尝试的策略。 
		•	java中的Compare and Swap即CAS ，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，
			而是被告知这次竞争中失败，并可以再次尝试。

	}
	
	什么是CAS操作，缺点是什么？：{
		CAS的基本思路就是，如果这个地址上的值和期望的值相等，则给其赋予新值，否则不做任何事儿，但是要返回原值是多少。
			每一个CAS操作过程都包含三个运算符：一个内存地址V，一个期望的值A和一个新值B，操作的时候如果这个地址上存放的值等于这个期望的值A，
			则将地址上的值赋为新值B，否则不做任何操作。
		CAS缺点： 
		ABA问题：
		比如说一个线程one从内存位置V中取出A，这时候另一个线程two也从内存中取出A，并且two进行了一些操作变成了B，然后two又将V位置的数据变成A，
			这时候线程one进行CAS操作发现内存中仍然是A，然后one操作成功。尽管线程one的CAS操作成功，但可能存在潜藏的问题。
			从Java1.5/开始JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。 
		循环时间长开销大： 
		对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的CPU资源，效率低于synchronized。 
		只能保证一个共享变量的原子操作： 
		当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁。

	}
	
	SynchronizedMap和ConcurrentHashMap有什么区别？：{
		SynchronizedMap一次锁住整张表来保证线程安全，所以每次只能有一个线程来访为map。
		ConcurrentHashMap使用分段锁来保证在多线程下的性能。

	}
	
	写时复制容器可以用于什么应用场景？：{
		CopyOnWrite并发容器用于对于绝大部分访问都是读，且只是偶尔写的并发场景。比如白名单，黑名单，商品类目的访问和更新场景。
		透露的思想 
		读写分离，读和写分开 
		最终一致性 
		使用另外开辟空间的思路，来解决并发冲突

	}
	
	volatile有什么用？能否用一句话说明下volatile的应用场景？：{
		volatile保证内存可见性和禁止指令重排。
		volatile用于多线程环境下的一写多读，或者无关联的多写。

	}
	
	为什么代码会重排序？：{
		在执行程序时，为了提供性能，处理器和编译器常常会对指令进行重排序，但是不能随意重排序，不是你想怎么排序就怎么排序，它需要满足以下两个条件：
		在单线程环境下不能改变程序运行的结果；
		存在数据依赖关系的不允许重排序

	}
	
	在java中wait和sleep方法的不同？：{
		最大的不同是在等待时wait会释放锁，而sleep一直持有锁。Wait通常被用于线程间交互，sleep通常被用于暂停执行。
	}
	
	一个线程运行时发生异常会怎样？：{
		如果异常没有被捕获该线程将会停止执行。hread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。
			当一个未捕获异常将造成线程中断的时候JVM会使用Thread.getUncaughtExceptionHandler()来查询线程的UncaughtExceptionHandler并将线程和
			异常作为参数传递给handler的uncaughtException()方法进行处理。
	}
	
	
	为什么wait, notify 和 notifyAll这些方法不在thread类里面？：{
		JAVA提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获得。如果线程需要等待某些锁那么调用对象中的wait()方法就有意义了。
			如果wait()方法定义在Thread类中，线程正在等待的是哪个锁就不明显了。简单的说，由于wait，notify和notifyAll都是锁级别的操作，
			所以把他们定义在Object类中因为锁属于对象。
	}
	
	什么是ThreadLocal变量？：{
		ThreadLocal是Java里一种特殊的变量。每个线程都有一个ThreadLocal就是每个线程都拥有了自己独立的一个变量，竞争条件被彻底消除了。
	}
	
	Java中interrupted 和 isInterrupted方法的区别？：{
		interrupted() 和 isInterrupted()的主要区别是前者会将中断状态清除而后者不会。Java多线程的中断机制是用内部标识来实现的，
			调用Thread.interrupt()来中断一个线程就会设置中断标识为true。当中断线程调用静态方法Thread.interrupted()来检查中断状态时，
			中断状态会被清零。而非静态方法isInterrupted()用来查询其它线程的中断状态且不会改变中断状态标识。
	}
	
	为什么wait和notify方法要在同步块中调用？：{
		主要是因为Java API强制要求这样做，如果你不这么做，你的代码会抛出IllegalMonitorStateException异常。
	}
	
	为什么你应该在循环中检查等待条件?：{
		处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。
			因此，当一个等待线程醒来时，不能认为它原来的等待状态仍然是有效的，在notify()方法调用之后和等待线程醒来之前这段时间它可能会改变。
			这就是在循环中使用wait()方法效果更好的原因
	}
	
	怎么检测一个线程是否拥有锁？：{
		在java.lang.Thread中有一个方法叫holdsLock()，它返回true如果当且仅当当前线程拥有某个具体对象的锁。
	}
	
	你如何在Java中获取线程堆栈？：{
		kill -3 [java pid] 
		不会在当前终端输出，它会输出到代码执行的或指定的地方去。比如，kill -3 tomcat pid, 输出堆栈到log目录下。
		Jstack [java pid] 
		这个比较简单，在当前终端显示，也可以重定向到指定文件中。
		或者使用Java提供的拟机线程系统的管理接口ManagementFactory.getThreadMXBean()。

	}
	
	Java线程池中submit() 和 execute()方法有什么区别？：{
		两个方法都可以向线程池提交任务，execute()方法的返回类型是void，它定义在Executor接口中。
		而submit()方法可以返回持有计算结果的Future对象，它定义在ExecutorService接口中，它扩展了Executor接口

	}
	
	你对线程优先级的理解是什么？：{
		每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实现，
			这个实现是和操作系统相关的(OS dependent)。我们可以定义线程的优先级，但是这并不能保证高优先级的线程会在低优先级的线程前执行。
			线程优先级是一个int变量(从1-10)，1代表最低优先级，10代表最高优先级。
		java的线程优先级调度会委托给操作系统去处理，所以与具体的操作系统优先级有关，如非特别需要，一般无需设置线程优先级。

	}
	
	你如何确保main()方法所在的线程是Java 程序最后结束的线程？：{
		可以使用Thread类的join()方法（或者CountDownLatch工具类）来确保所有程序创建的线程在main()方法退出前结束。
	}
	

	为什么Thread类的sleep()和yield ()方法是静态的？：{
		Thread类的sleep()和yield()方法将在当前正在执行的线程上运行。所以在其他处于等待状态的线程上调用这些方法是没有意义的。这就是为什么这些方法是静态的。
			它们可以在当前正在执行的线程中工作，并避免程序员错误的认为可以在其他非运行线程调用这些方法。
	}
	
	现在有T1、T2、T3三个线程，你怎样保证T2在T1执行完后执行，T3在T2执行完后执行？：{
		可以用join方法实现。
	}
	
	你需要实现一个高效的缓存，它允许多个用户读，但只允许一个用户写，以此来保持它的完整性，你会怎样去实现它？：{
		volatile关键字，读写锁，写时复制等等都可以实现。
	}
	
	用Java实现阻塞队列：{
		见作业答案：包cn.enjoyedu.ch5.answer下
	}
	
	用Java写代码来解决生产者——消费者问题。：{
		阻塞队列实现即可，也可以用wait和notify来解决这个问题，或者用Semaphore
	}
	
	用Java编程一个会导致死锁的程序，你将怎么解决？：{
		参见代码cn.enjoyedu.ch7. NormalDeadLock，如何解决死锁，参见笔记。
	}
	
	 Java中如何停止一个线程？：{
		使用共享变量的方式 
		在这种方式中，之所以引入共享变量，是因为该变量可以被多个执行相同任务的线程用来作为是否中断的信号，通知中断线程的执行。
		使用interrupt方法终止线程 
		如果一个线程由于等待某些事件的发生而被阻塞，又该怎样停止该线程呢？比如当一个线程由于需要等候键盘输入而被阻塞，或者调用Thread.join()方法，
			或者Thread.sleep()方法，在网络中调用ServerSocket.accept()方法，或者调用了DatagramSocket.receive()方法时，都有可能导致线程阻塞，
			使线程处于处于不可运行状态时，即使主程序中将该线程的共享变量设置为true，但该线程此时根本无法检查循环标志，当然也就无法立即中断。
			所以应该尽量使用Thread提供的interrupt()方法，因为该方法虽然不会中断一个正在运行的线程，但是它可以使一个被阻塞的线程抛出一个中断异常，
			从而使线程提前结束阻塞状态。

	}
	
	JVM中哪个参数是用来控制线程的栈堆栈小的：{
		-Xss
	}
	
	如果同步块内的线程抛出异常会发生什么？：{
		会
	}
	
	
	单例模式的双重检查实现是什么？为什么并不安全？如何在Java中创建线程安全的Singleton？：{
		实现参见cn.enjoyedu.ch7.dcl. SingleDcl，不安全的根本原因是重排序会导致未初始化完成的对象可以被其他线程看见而导致错误。
			创建安全的单例模式有：延迟占位模式、在声明的时候就new这个类的实例、枚举
	}
	
	写出3条你遵循的多线程最佳实践：{
		给你的线程起个有意义的名字。 这样可以方便找bug或追踪。OrderProcessor, QuoteProcessor or TradeProcessor 这种名字比 Thread-1. Thread-2 and Thread-3 好多了，
			给线程起一个和它要完成的任务相关的名字，所有的主要框架甚至JDK都遵循这个最佳实践。
		避免锁定和缩小同步的范围 锁花费的代价高昂且上下文切换更耗费时间空间，试试最低限度的使用同步和锁，缩小临界区。因此相对于同步方法我更喜欢同步块，
			它给我拥有对锁的绝对控制权。
		多用同步类少用wait 和 notify 首先，CountDownLatch, Semaphore, CyclicBarrier 和 Exchanger 这些同步类简化了编码操作，而用wait和notify很难实现对复杂控制流的控制。
			其次，这些类是由最好的企业编写和维护在后续的JDK中它们还会不断优化和完善，使用这些更高等级的同步工具你的程序可以不费吹灰之力获得优化。
		多用并发集合少用同步集合 这是另外一个容易遵循且受益巨大的最佳实践，并发集合比同步集合的可扩展性更好，所以在并发编程时使用并发集合效果更好。
		比如并发编程的黄金原则，尽量无锁化编程等等……..

	}
	
	请概述线程池的创建参数，怎么样合理配置一个线程池的参数？：{
		参见笔记中线程池一章的内容
	}
	
	请概述锁的公平和非公平，JDK内部是如何实现的。：{
		公平锁是指所有试图获得锁的线程按照获取锁的顺序依次获得锁，而非公平锁则是当前的锁状态没有被占用时,当前线程可以直接占用,而不需要等待。
			在实现上，非公平锁逻辑基本跟公平锁一致，唯一的区别是，当前线程不需要判断同步队列中是否有等待线程。
		非公平锁性能高于公平锁性能。首先，在恢复一个被挂起的线程与该线程真正运行之间存在着严重的延迟。而且，非公平锁能更充分的利用cpu的时间片，
			尽量的减少cpu空闲的状态时间。
		使用场景的话呢，其实还是和他们的属性一一相关，比如：如果业务中线程占用(处理)时间要远长于线程等待，那用非公平锁其实效率并不明显，
			但是用公平锁可以保证不会有线程被饿死。

	}
	
	请概述AQS：{
		是用来构建锁或者其他同步组件的基础框架，比如ReentrantLock、ReentrantReadWriteLock和CountDownLatch就是基于AQS实现的。
			它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作。它是CLH队列锁的一种变体实现。它可以实现2种同步方式：独占式，共享式。
		AQS的主要使用方式是继承，子类通过继承AQS并实现它的抽象方法来管理同步状态，同步器的设计基于模板方法模式，
			所以如果要实现我们自己的同步工具类就需要覆盖其中几个可重写的方法，如tryAcquire、tryReleaseShared等等。
		这样设计的目的是同步组件（比如锁）是面向使用者的，它定义了使用者与同步组件交互的接口（比如可以允许两个线程并行访问），隐藏了实现细节；
			同步器面向的是锁的实现者，它简化了锁的实现方式，屏蔽了同步状态管理、线程的排队、等待与唤醒等底层操作。这样就很好地隔离了使用者和实现者所需关注的领域。
		在内部，AQS维护一个共享资源state，通过内置的FIFO来完成获取资源线程的排队工作。该队列由一个一个的Node结点组成，每个Node结点维护一个prev引用和next引用，
			分别指向自己的前驱和后继结点，构成一个双端双向链表。
		同时与Condition相关的等待队列，节点类型也是Node，构成一个单向链表。

	}
	
	请概述Volatile：{
		volatile关键字的作用主要有两点：
		多线程主要围绕可见性和原子性两个特性而展开，使用volatile关键字修饰的变量，保证了其在多线程之间的可见性，即每次读取到volatile变量，一定是最新的数据。
			但是volatile不能保证操作的原子，对任意单个volatile变量的读/写具有原子性，但类似于++这种复合操作不具有原子性。。
		代码底层在执行时为了获取更好的性能会对指令进行重排序，多线程下可能会出现一些意想不到的问题。使用volatile则会对禁止重排序，当然这也一定程度上降低了代码执行效率。
		同时在内存语义上，当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存，当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。
			线程接下来将从主内存中读取共享变量。
		在Java中对于volatile修饰的变量，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序问题、强制刷新和读取。
		在具体实现上，volatile关键字修饰的变量会存在一个“lock:”的前缀。它不是一种内存屏障，但是它能完成类似内存屏障的功能。Lock会对CPU总线和高速缓存加锁，
			可以理解为CPU指令级的一种锁。
		同时该指令会将当前处理器缓存行的数据直接写会到系统内存中，且这个写回内存的操作会使在其他CPU里缓存了该地址的数据无效。

	}
	


}










